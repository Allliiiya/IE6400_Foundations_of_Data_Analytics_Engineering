{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b051c3",
   "metadata": {},
   "source": [
    "# IE6400 Foundations of Data Analytics Engineering\n",
    "# Fall 2023 \n",
    "### Module 4: Text Analysis\n",
    "#### - STUDENT VERSION -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca4774",
   "metadata": {},
   "source": [
    "### Text Analysis and Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing, or NLP, is a field of Artificial Intelligence (AI) that focuses on the interaction between computers and humans through natural language. The ultimate objective of NLP is to enable computers to understand, interpret, and respond to human languages in a valuable and meaningful way.\n",
    "\n",
    "#### Key Techniques in Text Analysis:\n",
    "\n",
    "#### 1. Tokenization\n",
    "Breaking down text into smaller fragments, known as tokens. Typically, tokens are words, but they can also be sentences or paragraphs.\n",
    "\n",
    "#### 2. Stopword Removal\n",
    "Eliminating common words (e.g., \"and\", \"the\", \"is\") that usually don't convey significant meaning in isolation.\n",
    "\n",
    "#### 3. Stemming and Lemmatization\n",
    "Both techniques aim to revert words to their base or root form. For instance, \"running\" might be transformed to \"run\". Stemming truncates prefixes and suffixes, while lemmatization considers context to convert the word to its meaningful base form.\n",
    "\n",
    "#### 4. Part-of-Speech Tagging\n",
    "Identifying the grammatical categories of words in the text, such as nouns, verbs, adjectives, etc.\n",
    "\n",
    "#### 5. Named Entity Recognition (NER)\n",
    "Classifying named entities in the text into predefined groups like person names, organizations, locations, etc.\n",
    "\n",
    "#### 6. Sentiment Analysis\n",
    "Determining the sentiment or emotion conveyed in the text, typically categorized as positive, negative, or neutral.\n",
    "\n",
    "#### 7. Topic Modeling\n",
    "Identifying prevalent topics in a text corpus. Latent Dirichlet Allocation (LDA) is a popular algorithm for this purpose.\n",
    "\n",
    "#### 8. Text Classification\n",
    "Assigning predefined categories or labels to a text based on its content.\n",
    "\n",
    "#### 9. Text Clustering\n",
    "Grouping texts that are similar in content.\n",
    "\n",
    "#### 10. Text Summarization\n",
    "Generating a concise and coherent summary of a more extensive text.\n",
    "\n",
    "#### 11. Word Embeddings\n",
    "Representing words in a dense vector space where semantically similar words are proximate. Methods like Word2Vec, GloVe, and FastText are popular for this.\n",
    "\n",
    "### Python Libraries for Text Analysis:\n",
    "\n",
    "Python boasts a plethora of libraries tailored for text analysis:\n",
    "\n",
    "- **NLTK (Natural Language Toolkit)**: A comprehensive toolkit for natural language processing.\n",
    "- **spaCy**: Known for its speed and precision, it's a high-performance library for NLP tasks.\n",
    "- **TextBlob**: Built atop NLTK and Pattern, offering a straightforward API for common NLP tasks.\n",
    "- **Gensim**: Renowned for topic modeling and word embedding tasks.\n",
    "- **Scikit-learn**: While primarily a machine learning library, it provides tools for text processing and modeling.\n",
    "\n",
    "In essence, text analysis in Python encompasses a broad spectrum of techniques to process, analyze, and extract insights from textual data. Python's rich ecosystem makes it an ideal choice for various NLP and text analytics tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae375611",
   "metadata": {},
   "source": [
    "### Word Frequency Analysis\n",
    "\n",
    "Word frequency analysis involves counting how often each word appears in a document or a set of documents. It's a fundamental technique in text analysis, often used to understand the main themes or topics in a text, or as a preprocessing step for more advanced text analysis tasks.\n",
    "\n",
    "#### Why is Word Frequency Analysis Important?\n",
    "\n",
    "1. **Identifying Key Themes**: By examining which words appear most frequently, we can often identify the main themes or topics of a document.\n",
    "   \n",
    "2. **Data Preprocessing**: Word frequencies can be used to filter out common but uninformative words (stopwords) or to identify important keywords to retain for further analysis.\n",
    "\n",
    "3. **Visualization**: Word frequency counts can be visualized in word clouds or bar charts, providing a quick and intuitive overview of the content of a text.\n",
    "\n",
    "4. **Feature Extraction for Machine Learning**: In text classification tasks, word frequencies (or related measures, like TF-IDF) can be used to convert text into a numerical format suitable for machine learning algorithms.\n",
    "\n",
    "### How to Perform Word Frequency Analysis in Python:\n",
    "\n",
    "#### 1. Tokenization\n",
    "The first step is to break the text into individual words or tokens.\n",
    "\n",
    "#### 2. Cleaning\n",
    "Remove punctuation, convert all words to lowercase (to ensure that words like \"The\" and \"the\" are counted as the same word), and remove stopwords.\n",
    "\n",
    "#### 3. Count Frequencies\n",
    "Use Python's collections library or specialized libraries like NLTK or spaCy to count how often each word appears.\n",
    "\n",
    "#### 4. Visualization (Optional)\n",
    "Visualize the most common words in a bar chart or a word cloud.\n",
    "\n",
    "#### Challenges:\n",
    "\n",
    "- **Stopwords**: Common words like \"and\", \"the\", and \"is\" can appear very frequently but are often not informative on their own. They can be removed using a predefined list of stopwords.\n",
    "  \n",
    "- **Stemming/Lemmatization**: Different forms of the same word (e.g., \"run\" and \"running\") can be counted separately. Stemming or lemmatization can be used to reduce words to their root form.\n",
    "\n",
    "- **Context**: Word frequency analysis doesn't consider the order of words, so it can miss nuances in meaning or context.\n",
    "\n",
    "In conclusion, word frequency analysis is a powerful and straightforward technique for understanding the content of a text. While it has some limitations, it can provide valuable insights, especially when combined with other text analysis methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ec281",
   "metadata": {},
   "source": [
    "#### Exercise 1 Word Frequency Analysis Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Data Loading and Exploration\n",
    "df = pd.read_csv('winemag-data-130k-v2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd6e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10017dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Cleaning\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = df['description'].str.lower().str.cat(sep=' ')\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "\n",
    "# Just show 10 first tokens\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Just show 10 first filtered_tokens\n",
    "filtered_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df542acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Word Frequency Analysis\n",
    "from collections import Counter\n",
    "\n",
    "word_freq = Counter(filtered_tokens)\n",
    "top_words = word_freq.most_common(10)\n",
    "print(top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666dad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words, counts = zip(*top_words)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(words, counts, color='purple')\n",
    "plt.title('Top 10 Most Frequent Words in Wine Reviews')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1af9b",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "Stemming and lemmatization are both techniques used in Natural Language Processing (NLP) to reduce words to their base or root form. While they aim to achieve similar goals, they operate on different principles and methods.\n",
    "\n",
    "### Stemming\n",
    "\n",
    "Stemming is the process of reducing a word to its base or root form by removing the suffixes (or in some cases prefixes). For example, the stem of the word \"running\" might be \"run\".\n",
    "\n",
    "#### Algorithms for Stemming:\n",
    "\n",
    "1. **Porter Stemming Algorithm (PorterStemmer)**: \n",
    "   - Developed by Martin Porter in 1980.\n",
    "   - It uses a set of heuristic rules to transform words.\n",
    "   - Example: \"running\" -> \"run\", \"flies\" -> \"fli\".\n",
    "\n",
    "2. **Lancaster Stemming Algorithm (LancasterStemmer)**:\n",
    "   - It is more aggressive than the Porter stemming algorithm.\n",
    "   - It has a set of iterative rules to convert words.\n",
    "   - Example: \"running\" -> \"run\", \"flies\" -> \"fly\".\n",
    "\n",
    "3. **Snowball Stemmer**:\n",
    "   - Also known as the \"Porter2\" stemming algorithm.\n",
    "   - It is an improvement over the original Porter stemmer and supports multiple languages.\n",
    "   - Example: \"running\" -> \"run\", \"flies\" -> \"fli\".\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "Lemmatization is the process of reducing a word to its base or dictionary form. It involves looking up a word in a lexicon and returning its lemma or canonical form. For example, the lemma of the word \"running\" is \"run\", and the lemma of \"better\" is \"good\".\n",
    "\n",
    "#### Algorithms for Lemmatization:\n",
    "\n",
    "1. **WordNet Lemmatizer**:\n",
    "   - Uses the WordNet lexical database.\n",
    "   - It returns the base or dictionary form of a word.\n",
    "   - Example: \"running\" -> \"run\", \"better\" -> \"good\".\n",
    "\n",
    "2. **Spacy Lemmatizer**:\n",
    "   - Uses the Spacy NLP library.\n",
    "   - It is based on detailed linguistic annotations.\n",
    "   - Example: \"running\" -> \"run\", \"better\" -> \"good\".\n",
    "\n",
    "3. **TextBlob Lemmatizer**:\n",
    "   - Uses the WordNet lexical database.\n",
    "   - It is a simple API over the NLTK library.\n",
    "   - Example: \"running\" -> \"run\", \"better\" -> \"good\".\n",
    "\n",
    "In summary, while stemming might produce non-real words (like \"fli\" for \"flies\"), lemmatization always produces real words. The choice between stemming and lemmatization depends on the application and the level of precision required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a2c28",
   "metadata": {},
   "source": [
    "#### Exercise 2 Stemming Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c4940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wine_df = pd.read_csv('winemag-data-130k-v2.csv')\n",
    "food_df = pd.read_csv('Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wine Reviews:\")\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amazon Fine Food Reviews:\")\n",
    "food_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3185a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Truncate datasets to first 100 records\n",
    "wine_df = wine_df.head(100)\n",
    "food_df = food_df.head(100)\n",
    "\n",
    "wine_text = wine_df['description'].str.lower().str.cat(sep=' ')\n",
    "food_text = food_df['Text'].str.lower().str.cat(sep=' ')\n",
    "\n",
    "wine_text = wine_text.translate(str.maketrans('', '', string.punctuation))\n",
    "food_text = food_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "wine_tokens = word_tokenize(wine_text)\n",
    "food_tokens = word_tokenize(food_text)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_wine_tokens = [word for word in wine_tokens if word not in stop_words]\n",
    "filtered_food_tokens = [word for word in food_tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c109c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_wine_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2595d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_food_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab589f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "wine_porter_stems = [porter.stem(word) for word in filtered_wine_tokens]\n",
    "food_porter_stems = [porter.stem(word) for word in filtered_food_tokens]\n",
    "\n",
    "wine_lancaster_stems = [lancaster.stem(word) for word in filtered_wine_tokens]\n",
    "food_lancaster_stems = [lancaster.stem(word) for word in filtered_food_tokens]\n",
    "\n",
    "wine_snowball_stems = [snowball.stem(word) for word in filtered_wine_tokens]\n",
    "food_snowball_stems = [snowball.stem(word) for word in filtered_food_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b16f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_stems(tokens, porter_stems, lancaster_stems, snowball_stems, title):\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    x = range(len(tokens))\n",
    "    plt.scatter(x, tokens, color='blue', label='Original Tokens')\n",
    "    plt.scatter(x, porter_stems, color='red', label='Porter Stems')\n",
    "    plt.scatter(x, lancaster_stems, color='green', label='Lancaster Stems')\n",
    "    plt.scatter(x, snowball_stems, color='yellow', label='Snowball Stems')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize stems for the first 10 tokens\n",
    "plot_stems(filtered_wine_tokens[:10], wine_porter_stems[:10], wine_lancaster_stems[:10], wine_snowball_stems[:10], 'Wine Reviews Stemming Comparison')\n",
    "plot_stems(filtered_food_tokens[:10], food_porter_stems[:10], food_lancaster_stems[:10], food_snowball_stems[:10], 'Amazon Fine Food Reviews Stemming Comparison')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd2612",
   "metadata": {},
   "source": [
    "#### Exercise 3 Lemmatization Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('winemag-data-130k-v2.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7559fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Truncate dataset to first 100 records\n",
    "df = df.head(100)\n",
    "\n",
    "text = df['description'].str.lower().str.cat(sep=' ')\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Just show 10 first filtered_tokens\n",
    "filtered_tokens[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "# Just show 10 first tokens\n",
    "lemmatized_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_lemmatization(filtered_tokens, lemmatized_tokens, title):\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    x = range(len(filtered_tokens))\n",
    "    plt.scatter(x, filtered_tokens, color='blue', label='Original Tokens')\n",
    "    plt.scatter(x, lemmatized_tokens, color='red', label='Lemmatized Tokens')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize lemmatization for the first 10 tokens\n",
    "plot_lemmatization(filtered_tokens[:10], lemmatized_tokens[:10], 'Effects of Lemmatization on Wine Reviews')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73853da7",
   "metadata": {},
   "source": [
    "### Zipf's Law\n",
    "\n",
    "Zipf's Law is an empirical law that describes the distribution of word frequencies in natural languages. It states that the frequency of any word in a corpus is inversely proportional to its rank in the frequency table. In other words, the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, and so on.\n",
    "\n",
    "Mathematically, Zipf's Law can be represented as:\n",
    "\n",
    "> $f = \\frac{c}{r}$\n",
    "\n",
    "Where:\n",
    "- \\( f \\) is the frequency of the word.\n",
    "- \\( r \\) is the rank of the word.\n",
    "- \\( c \\) is a constant.\n",
    "\n",
    "#### Key Points:\n",
    "\n",
    "1. **Word Frequency Distribution**:\n",
    "   - In a typical corpus, a few words (like \"the\", \"and\", \"of\") occur very frequently, while the majority of words occur rarely.\n",
    "\n",
    "2. **Log-Log Plot**:\n",
    "   - When plotting the log of the frequency against the log of the rank, Zipf's Law produces a straight line with a slope of approximately -1.\n",
    "\n",
    "3. **Applications**:\n",
    "   - Zipf's Law has been observed in various phenomena, not just language. It applies to the distribution of city populations, income rankings, and even the number of citations received by academic papers.\n",
    "\n",
    "4. **Variations**:\n",
    "   - While Zipf's Law holds true for many corpora, there are variations. Some corpora may not follow the exact \\( f = \\frac{c}{r} \\) distribution, but they often exhibit a similar hyperbolic distribution.\n",
    "\n",
    "5. **Implications**:\n",
    "   - Zipf's Law has implications for linguistics, information theory, and even the design of search engines and databases.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Consider a corpus with the word \"the\" being the most frequent word, occurring 1,000 times. According to Zipf's Law:\n",
    "- The 2nd most frequent word will occur approximately 500 times.\n",
    "- The 3rd most frequent word will occur approximately 333 times.\n",
    "- The 4th most frequent word will occur approximately 250 times, and so on.\n",
    "\n",
    "In conclusion, Zipf's Law provides a fascinating insight into the patterns of word distribution in natural languages and has been a topic of interest for researchers in various fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38250eb",
   "metadata": {},
   "source": [
    "#### Exercise 4 Zipf's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f0b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('winemag-data-130k-v2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c8fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = df['description'].str.lower().str.cat(sep=' ')\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "tokens = word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eac23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_freq = Counter(tokens)\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ranks = np.arange(1, len(sorted_word_freq)+1)\n",
    "frequencies = [freq for word, freq in sorted_word_freq]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(ranks, frequencies, marker=\"o\")\n",
    "plt.title(\"Zipf's Law Visualization\")\n",
    "plt.xlabel(\"Rank of the word\")\n",
    "plt.ylabel(\"Frequency of the word\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8161d874",
   "metadata": {},
   "source": [
    "### N-grams, Unigrams, and Bigrams\n",
    "\n",
    "In the context of natural language processing and text analysis, N-grams refer to a contiguous sequence of 'N' items (typically words) from a given sample of text or speech. They are used to capture the language structure, such as word patterns and phrases, from a text corpus.\n",
    "\n",
    "### Unigrams\n",
    "\n",
    "- **Definition**: A unigram is a single word or item from a text. It is the simplest form of N-gram, where N=1.\n",
    "- **Example**: The sentence \"I love programming\" contains the unigrams \"I\", \"love\", and \"programming\".\n",
    "\n",
    "### Bigrams\n",
    "\n",
    "- **Definition**: A bigram is a sequence of two adjacent words or items from a text. It is an N-gram where N=2.\n",
    "- **Example**: The sentence \"I love programming\" contains the bigrams \"I love\" and \"love programming\".\n",
    "\n",
    "### N-grams\n",
    "\n",
    "- **Definition**: An N-gram is a sequence of 'N' words or items from a text. The value of N can be any positive integer, and it determines the number of words or items in the sequence.\n",
    "- **Example**: In the sentence \"I love programming\", the 3-gram (or trigram) is \"I love programming\".\n",
    "\n",
    "#### Key Points:\n",
    "\n",
    "1. **Usage**: \n",
    "   - N-grams are widely used in text processing tasks such as text prediction, spelling correction, and sentiment analysis.\n",
    "   \n",
    "2. **Higher-Order N-grams**:\n",
    "   - As the value of N increases, the N-grams capture more context but also become sparser in the text. For instance, 4-grams and 5-grams provide more context than bigrams but are less frequent in a typical corpus.\n",
    "\n",
    "3. **Limitations**:\n",
    "   - While N-grams capture local word patterns, they do not capture long-distance dependencies between words or the overall sentence structure.\n",
    "\n",
    "4. **Smoothing Techniques**:\n",
    "   - Due to the sparsity of higher-order N-grams in a corpus, smoothing techniques are often applied in probabilistic language models to handle unseen N-grams.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Consider the sentence \"I love coding in Python\".\n",
    "\n",
    "- **Unigrams**: \"I\", \"love\", \"coding\", \"in\", \"Python\"\n",
    "- **Bigrams**: \"I love\", \"love coding\", \"coding in\", \"in Python\"\n",
    "- **Trigrams**: \"I love coding\", \"love coding in\", \"coding in Python\"\n",
    "\n",
    "In conclusion, N-grams provide a way to represent and capture the structure of text, making them a fundamental concept in many natural language processing tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b1575c",
   "metadata": {},
   "source": [
    "#### Exercise 5 N-grams, Unigrams, and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd6264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('winemag-data-130k-v2.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d842dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = df['description'].str.lower().str.cat(sep=' ')\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "unigrams = list(ngrams(filtered_tokens, 1))\n",
    "bigrams = list(ngrams(filtered_tokens, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0253d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_unigrams = Counter(unigrams).most_common(10)\n",
    "top_bigrams = Counter(bigrams).most_common(10)\n",
    "\n",
    "def plot_ngrams(ngrams_list, title):\n",
    "    ngrams, counts = zip(*ngrams_list)\n",
    "    ngrams = [\" \".join(gram) for gram in ngrams]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(ngrams, counts, color='purple')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "plot_ngrams(top_unigrams, 'Top 10 Unigrams')\n",
    "plot_ngrams(top_bigrams, 'Top 10 Bigrams')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6299d96a",
   "metadata": {},
   "source": [
    "### Word Cloud\n",
    "\n",
    "A Word Cloud (also known as a tag cloud or text cloud) is a visual representation of text data where the importance or frequency of each word is represented by its size and/or color. In a word cloud, the more frequently a word appears in the source text, the larger and bolder it appears in the cloud.\n",
    "\n",
    "#### Key Features:\n",
    "\n",
    "1. **Visualization**:\n",
    "   - Word Clouds provide a quick visual summary of a large amount of text, highlighting the most prominent terms.\n",
    "\n",
    "2. **Customization**:\n",
    "   - The appearance, color scheme, and shape of a word cloud can often be customized to fit specific aesthetics or themes.\n",
    "\n",
    "3. **Interactivity**:\n",
    "   - Some word cloud tools allow for interactive features, such as hovering over a word to see its frequency or clicking on a word to filter related content.\n",
    "\n",
    "4. **Applications**:\n",
    "   - Word Clouds are commonly used in data analysis, content summaries, presentations, and website design. They are particularly popular for analyzing feedback, reviews, and open-ended survey responses.\n",
    "\n",
    "#### How to Create a Word Cloud:\n",
    "\n",
    "1. **Text Data**:\n",
    "   - Start with a collection of text data, such as articles, reviews, or any other textual content.\n",
    "\n",
    "2. **Text Preprocessing**:\n",
    "   - Clean the text by removing stop words (common words like \"and\", \"the\", \"is\", etc.), punctuation, and any other unwanted characters.\n",
    "   - Optionally, apply stemming or lemmatization to reduce words to their base form.\n",
    "\n",
    "3. **Word Frequency**:\n",
    "   - Calculate the frequency of each word in the text.\n",
    "\n",
    "4. **Visualization**:\n",
    "   - Use a word cloud generator or library (e.g., the `WordCloud` library in Python) to create the visual representation based on word frequencies.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Consider the feedback from a product review: \"The product is amazing. I love the design and the functionality. The team did an amazing job.\"\n",
    "\n",
    "In a word cloud representation, the words \"amazing\", \"love\", \"design\", \"functionality\", and \"team\" might appear larger because they convey the main sentiments and topics of the feedback.\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "Word Clouds offer an intuitive and visually appealing way to understand the main themes or sentiments in a large dataset. However, while they are useful for a quick overview, they lack the depth and context provided by more detailed textual analysis methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6048e0",
   "metadata": {},
   "source": [
    "#### Exercise 6 WordCloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bfd615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('winemag-data-130k-v2.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a1f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "text = df['description'].str.lower().str.cat(sep=' ')\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c360c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c598878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('WordCloud for Wine Reviews')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7c32c",
   "metadata": {},
   "source": [
    "### Fuzzy Matching in Text Analysis\n",
    "\n",
    "Fuzzy matching is a technique used in text analysis to find strings that are approximately equal to a given pattern. Unlike exact string matching, where the strings must be identical, fuzzy matching allows for minor discrepancies, such as typos, variations in spelling, or differences in word order.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "1. **Edit Distance**:\n",
    "   - A measure of the similarity between two strings. It represents the minimum number of operations (insertions, deletions, or substitutions) required to transform one string into another.\n",
    "   - Common algorithms include the Levenshtein distance and the Damerau-Levenshtein distance.\n",
    "\n",
    "2. **Token-Based Matching**:\n",
    "   - Involves breaking the text into tokens (words, phrases, or n-grams) and comparing the sets of tokens. The Jaccard similarity coefficient is a common metric used in token-based matching.\n",
    "\n",
    "3. **Phonetic Matching**:\n",
    "   - Matches strings based on their phonetic similarity rather than their written form. Algorithms like Soundex and Metaphone are used to encode words into a phonetic representation.\n",
    "\n",
    "#### Applications:\n",
    "\n",
    "1. **Data Cleaning**:\n",
    "   - Identifying and correcting typos or variations in data entries.\n",
    "\n",
    "2. **Record Linkage**:\n",
    "   - Merging records from different databases that refer to the same entity but have slight variations in naming.\n",
    "\n",
    "3. **Search Engines**:\n",
    "   - Improving search results by returning relevant items even if the search query contains typos or variations.\n",
    "\n",
    "4. **Natural Language Processing**:\n",
    "   - Matching synonyms or semantically similar words in text analysis tasks.\n",
    "\n",
    "#### Tools and Libraries:\n",
    "\n",
    "- **Python**:\n",
    "   - Libraries such as `fuzzywuzzy`, `textdistance`, and `difflib` provide functions for fuzzy string matching.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Consider a database with the entry \"Microsoft Corporation\". Using fuzzy matching, we can identify the following entries as being similar:\n",
    "- \"Microsft Corporation\" (typo)\n",
    "- \"Microsoft Corp\" (abbreviation)\n",
    "- \"Microsoft Incorporated\" (variation)\n",
    "\n",
    "#### Limitations:\n",
    "\n",
    "- Fuzzy matching can be computationally intensive, especially for large datasets.\n",
    "- Determining the appropriate threshold for a \"match\" can be subjective and may require domain knowledge.\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "Fuzzy matching is a powerful technique for text analysis, especially in scenarios where data may have inconsistencies or variations. It provides flexibility in matching strings and can significantly improve the accuracy and quality of data processing tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d1f293",
   "metadata": {},
   "source": [
    "#### Exercise 7 Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('winemag-data-130k-v2.csv')\n",
    "df['winery'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb634a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d3a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "wineries = df['winery'].unique().tolist()\n",
    "top_matches = process.extract(\"Hill\", wineries, limit=10)\n",
    "print(top_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a953b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "names, scores = zip(*top_matches)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(names, scores, color='teal')\n",
    "plt.xlabel('Fuzzy Matching Score')\n",
    "plt.ylabel('Winery Names')\n",
    "plt.title('Top Fuzzy Matches for \"Hill\"')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b3204",
   "metadata": {},
   "source": [
    "#### Exercise 8 Fuzzy Matching Dataframe Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8bbd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data1 = {\n",
    "    'First Name': ['John', 'Jane', 'Robert', 'Alice', 'Steve'],\n",
    "    'Last Name': ['Doe', 'Smith', 'Johnson', 'Williams', 'Brown']\n",
    "}\n",
    "\n",
    "data2 = {\n",
    "    'First Name': ['Jon', 'Janet', 'Rob', 'Alicia', 'Steven'],\n",
    "    'Last Name': ['Do', 'Smit', 'Johnsen', 'William', 'Browne']\n",
    "}\n",
    "\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "print(df1)\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77fea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def get_match(row, master_df, column_name, threshold=80):\n",
    "    best_match = None\n",
    "    highest_score = 0\n",
    "    for item in master_df[column_name]:\n",
    "        score = fuzz.ratio(row[column_name], item)\n",
    "        if score > threshold and score > highest_score:\n",
    "            highest_score = score\n",
    "            best_match = item\n",
    "    return best_match\n",
    "\n",
    "df2['Matched Last Name'] = df2.apply(get_match, master_df=df1, column_name='Last Name', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31749b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df1, df2, left_on='Last Name', right_on='Matched Last Name', suffixes=('_Original', '_Altered'))\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(merged_df['Last Name_Original'], merged_df.index, color='blue', label='Original Last Names')\n",
    "plt.bar(merged_df['Last Name_Altered'], merged_df.index, color='red', alpha=0.5, label='Altered Last Names')\n",
    "plt.yticks(merged_df.index, merged_df['First Name_Original'])\n",
    "plt.xlabel('Last Names')\n",
    "plt.ylabel('First Names')\n",
    "plt.title('Comparison of Original and Altered Last Names')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b1cced",
   "metadata": {},
   "source": [
    "### Sentiment Analysis \n",
    "\n",
    "Sentiment Analysis, often referred to as opinion mining, is a subfield of Natural Language Processing (NLP) that focuses on identifying and categorizing opinions expressed in text, especially in order to determine whether the writer's attitude towards a particular topic, product, or service is positive, negative, or neutral. The significance of sentiment analysis lies in its ability to gauge public opinion, conduct nuanced market research, monitor brand and product reputation, and understand customer experiences.\n",
    "\n",
    "#### How Sentiment Analysis Works\n",
    "\n",
    "Sentiment analysis typically involves the following steps:\n",
    "\n",
    "1. **Data Collection**: Gathering data, usually text, from various sources like websites, online forums, social media platforms, and customer reviews.\n",
    "\n",
    "2. **Preprocessing**: Cleaning and preparing the text data for analysis. This can include removing noise such as special characters and numbers, standardizing text, tokenization (breaking text into individual words or phrases), and stemming or lemmatization (reducing words to their base or root form).\n",
    "\n",
    "3. **Feature Extraction**: Transforming text into a format that can be analyzed by machine learning algorithms. This often involves creating a bag-of-words model or using word embeddings that capture semantic meanings of words.\n",
    "\n",
    "4. **Model Training**: Using machine learning algorithms to train a sentiment analysis model on a labeled dataset, where the sentiment for each text snippet is known. This could involve supervised learning techniques such as logistic regression, support vector machines, or neural networks.\n",
    "\n",
    "5. **Classification**: Applying the trained model to new, unseen text to classify the sentiment. The output is usually a score that indicates the polarity of sentiment, or a label such as \"positive,\" \"negative,\" or \"neutral.\"\n",
    "\n",
    "6. **Interpretation**: Analyzing the results to draw insights. For instance, a company might analyze customer feedback to determine overall satisfaction with a product or service.\n",
    "\n",
    "#### Applications of Sentiment Analysis\n",
    "\n",
    "- **Business Analytics**: Companies use sentiment analysis to understand customer sentiment towards products or services, often through analysis of online reviews and social media conversations.\n",
    "\n",
    "- **Market Research**: Sentiment analysis helps in gauging public opinion on various topics, brands, or products, which can inform marketing strategies.\n",
    "\n",
    "- **Politics**: During elections, sentiment analysis can be used to assess public opinion on candidates or issues.\n",
    "\n",
    "- **Customer Service**: Automatically categorizing customer support tickets based on sentiment can help businesses prioritize and respond to urgent queries.\n",
    "\n",
    "#### Challenges in Sentiment Analysis\n",
    "\n",
    "- **Sarcasm and Irony**: Detecting sarcasm or irony in text can be difficult for algorithms, as they often require context and understanding of subtle language cues.\n",
    "\n",
    "- **Contextual Meaning**: Words can have different meanings in different contexts, which can lead to misinterpretation of sentiment.\n",
    "\n",
    "- **Language Nuances**: Sentiment analysis models must handle various linguistic nuances such as idioms, negations, and intensifiers.\n",
    "\n",
    "Despite these challenges, sentiment analysis remains a powerful tool in NLP, providing valuable insights across various domains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c1437d",
   "metadata": {},
   "source": [
    "#### Exercise 9 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ce6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('winemag-data-130k-v2.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14035f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2eda99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "df['sentiment'] = df['description'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sentiment_counts.plot(kind='bar', color=['green', 'blue', 'red'])\n",
    "plt.title('Sentiment Distribution in Wine Reviews')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e0690e",
   "metadata": {},
   "source": [
    "### Topic Modeling \n",
    "\n",
    "Topic Modeling is an unsupervised machine learning technique in Natural Language Processing (NLP) that discovers the abstract \"topics\" that occur in a collection of documents. It is used to uncover hidden thematic structures in large textual corpora, categorize text documents into topics, and aid in the organization of large datasets by topic. This technique is particularly useful in digital libraries, information retrieval, and various content-based recommendation systems.\n",
    "\n",
    "#### How Topic Modeling Works\n",
    "\n",
    "Topic modeling involves the following steps:\n",
    "\n",
    "1. **Data Collection**: Assembling a corpus of text data that needs to be analyzed.\n",
    "\n",
    "2. **Preprocessing**: Cleaning the text data to remove noise, including punctuation, special characters, and numbers. It also involves tokenization, stop-word removal, stemming, and lemmatization.\n",
    "\n",
    "3. **Model Selection**: Choosing a statistical model for topic modeling. The most popular models include Latent Dirichlet Allocation (LDA), Non-negative Matrix Factorization (NMF), and Latent Semantic Analysis (LSA).\n",
    "\n",
    "4. **Model Training**: Applying the chosen model to the preprocessed text data to identify patterns and topics. The model will learn to assign topic distributions to documents and word distributions to topics.\n",
    "\n",
    "5. **Reviewing Topics**: After training, each topic is represented as a collection of terms with weights indicating their relevance to the topic. Analysts review these terms to interpret and label the topics.\n",
    "\n",
    "6. **Assigning Topics**: Documents are then categorized based on the topics they are most strongly associated with, according to the model.\n",
    "\n",
    "#### Applications of Topic Modeling\n",
    "\n",
    "- **Content Summarization**: Topic modeling can help summarize large volumes of text by identifying the main themes.\n",
    "\n",
    "- **Information Retrieval**: Enhancing search engines by indexing documents based on topics.\n",
    "\n",
    "- **Understanding Trends**: Analyzing social media or customer feedback to identify trending topics or issues.\n",
    "\n",
    "- **Recommender Systems**: Recommending articles, news, or products to users based on their interests in certain topics.\n",
    "\n",
    "#### Challenges in Topic Modeling\n",
    "\n",
    "- **Interpreting Topics**: Topics are clusters of words, and interpreting them to find a coherent theme can sometimes be subjective.\n",
    "\n",
    "- **Choosing the Number of Topics**: Determining the optimal number of topics can be difficult and often requires domain knowledge or iterative experimentation.\n",
    "\n",
    "- **Polysemy and Synonymy**: Words with multiple meanings (polysemy) and different words with similar meanings (synonymy) can affect the quality of the topics.\n",
    "\n",
    "- **Dynamic Topics**: Topics can change over time, and static models may not capture these changes effectively.\n",
    "\n",
    "Despite these challenges, topic modeling is a powerful tool in the NLP toolkit, providing insights into the themes and concepts present in large and unstructured text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19844607",
   "metadata": {},
   "source": [
    "#### Exercise 10 Topic Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f422bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings \n",
    "# Settings the warnings to be ignored \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('winemag-data-130k-v2.csv')\n",
    "df = df.head(100)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520030b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "df['processed_description'] = df['description'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fcb781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42dba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(df['processed_description'])\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in df['processed_description']]\n",
    "\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "lda_model = LdaModel(doc_term_matrix, num_topics=5, id2word=dictionary, passes=15)\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76762fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c32644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "import warnings \n",
    "# Settings the warnings to be ignored \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "vis_data = gensimvis.prepare(lda_model, doc_term_matrix, dictionary)\n",
    "pyLDAvis.display(vis_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c0f9b",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER) \n",
    "\n",
    "Named Entity Recognition (NER) is a key task in Natural Language Processing (NLP) that involves identifying and classifying named entities in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. NER is used to answer real-world questions like \"Who is mentioned in the text?\", \"Where are the different places discussed?\", or \"What specific dates are referenced?\"\n",
    "\n",
    "#### How NER Works\n",
    "\n",
    "NER systems typically follow these steps:\n",
    "\n",
    "1. **Tokenization**: Segmenting text into words, phrases, symbols, or other meaningful elements called tokens.\n",
    "\n",
    "2. **Part-of-Speech Tagging**: Assigning parts of speech to each token, such as noun, verb, adjective, etc., based on both its definition and its context.\n",
    "\n",
    "3. **Chunking**: Parsing and segmenting sentences into phrases, which can be used as input for NER.\n",
    "\n",
    "4. **Entity Identification**: Determining which tokens or phrases are named entities.\n",
    "\n",
    "5. **Classification**: Assigning a category to each identified entity, such as PERSON, ORGANIZATION, or LOCATION.\n",
    "\n",
    "6. **Post-processing**: Refining the output, potentially using additional resources like entity databases for disambiguation and validation.\n",
    "\n",
    "#### Techniques Used in NER\n",
    "\n",
    "- **Rule-Based Approaches**: Using handcrafted linguistic rules to identify named entities based on patterns.\n",
    "\n",
    "- **Statistical Models**: Leveraging models like Conditional Random Fields (CRFs), Hidden Markov Models (HMMs), or Support Vector Machines (SVMs) trained on annotated corpora.\n",
    "\n",
    "- **Deep Learning**: Applying neural network architectures, such as Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), or Transformer-based models like BERT, that can capture complex patterns and dependencies.\n",
    "\n",
    "#### Applications of NER\n",
    "\n",
    "- **Information Extraction**: Automatically extracting structured information from unstructured text sources.\n",
    "\n",
    "- **Content Classification**: Enhancing search and content discovery by tagging entities.\n",
    "\n",
    "- **Question Answering**: Identifying entities in questions to retrieve or generate accurate answers.\n",
    "\n",
    "- **Sentiment Analysis**: Determining the sentiment towards specific entities.\n",
    "\n",
    "- **Knowledge Graphs**: Populating knowledge bases with entities and their relationships.\n",
    "\n",
    "#### Challenges in NER\n",
    "\n",
    "- **Ambiguity**: A single entity name can refer to multiple unique entities (e.g., \"Jordan\" can refer to a person's name or a country).\n",
    "\n",
    "- **Variation**: An entity can be referred to in multiple ways (e.g., \"USA\" and \"United States of America\").\n",
    "\n",
    "- **Context Dependence**: The meaning and entity type can depend heavily on context.\n",
    "\n",
    "- **Domain Specificity**: Entities in specialized fields may require tailored approaches.\n",
    "\n",
    "Despite these challenges, NER continues to be a vital component of NLP, enabling machines to understand and process human language in a more structured and insightful way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520fc0cd",
   "metadata": {},
   "source": [
    "#### Exercise 11 Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a18d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('winemag-data-130k-v2.csv')\n",
    "df = df.head(100)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b85d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def extract_entities_nltk(text):\n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "    tree = ne_chunk(pos_tags)\n",
    "    named_entities = []\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() in ['GPE', 'PERSON', 'ORGANIZATION', 'DATE', 'LOCATION']:\n",
    "            entity = \" \".join([word for word, tag in subtree.leaves()])\n",
    "            named_entities.append((entity, subtree.label()))\n",
    "    return named_entities\n",
    "\n",
    "df['named_entities_nltk'] = df['description'].apply(extract_entities_nltk)\n",
    "print(df[['description', 'named_entities_nltk']].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7870ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "all_entities = [entity for sublist in df['named_entities_nltk'] for entity in sublist]\n",
    "entity_counts = Counter([entity[0] for entity in all_entities])\n",
    "\n",
    "common_entities = entity_counts.most_common(10)\n",
    "\n",
    "entities = [item[0] for item in common_entities]\n",
    "counts = [item[1] for item in common_entities]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(entities, counts, color='skyblue')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 10 Named Entities in Wine Reviews')\n",
    "plt.xlabel('Named Entities')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a7fc17",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87829017",
   "metadata": {},
   "source": [
    "#### Revised Date: November 18, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2369fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
