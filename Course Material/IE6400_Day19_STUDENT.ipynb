{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6066e1",
   "metadata": {},
   "source": [
    "# IE6400 Foundations of Data Analytics Engineering\n",
    "# Fall 2023 \n",
    "### Module 3: Clustering Methods Part -2\n",
    "#### - STUDENT VERSION -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031db9cd",
   "metadata": {},
   "source": [
    "### Clustering Methods\n",
    "\n",
    "Clustering is an unsupervised learning technique used to group similar data points into clusters. The goal is to partition a dataset such that points in the same group are more similar to each other than to those in other groups. There are several clustering methods, each suitable for different types of data and applications.\n",
    "\n",
    "#### 1. K-Means Clustering\n",
    "\n",
    "- **Description**: Partitions the data into \\(K\\) distinct non-overlapping clusters based on distances to the center of each cluster.\n",
    "- **Algorithm**:\n",
    "  1. Initialize \\(K\\) cluster centroids randomly.\n",
    "  2. Assign each data point to the nearest centroid.\n",
    "  3. Recompute centroids based on the current cluster assignments.\n",
    "  4. Repeat steps 2-3 until convergence.\n",
    "\n",
    "#### 2. Hierarchical Clustering\n",
    "\n",
    "- **Description**: Creates a tree of clusters. Can be agglomerative (bottom-up) or divisive (top-down).\n",
    "- **Algorithm**:\n",
    "  1. Treat each data point as a single cluster. (Total \\(n\\) clusters)\n",
    "  2. Find the two clusters that are closest and merge them.\n",
    "  3. Repeat step 2 until only one cluster remains.\n",
    "\n",
    "#### 3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "- **Description**: Clusters are dense regions in the data space, separated by areas of lower point density. Can find arbitrarily shaped clusters.\n",
    "- **Algorithm**:\n",
    "  1. For each point, count the number of points within a specified radius (\\( \\epsilon \\)).\n",
    "  2. If a point has more than a specified number (MinPts) of neighbors, it's a core point.\n",
    "  3. Cluster core points closer than \\( \\epsilon \\) and reach border points.\n",
    "  4. Points not in any cluster are treated as noise.\n",
    "\n",
    "#### 4. Mean Shift Clustering\n",
    "\n",
    "- **Description**: Based on centroid shifting to modes of the data density.\n",
    "- **Algorithm**:\n",
    "  1. Initialize centroids randomly.\n",
    "  2. Update centroids to the mean of data points within a given window.\n",
    "  3. Repeat step 2 until convergence.\n",
    "\n",
    "#### 5. Gaussian Mixture Models (GMM)\n",
    "\n",
    "- **Description**: Assumes data is generated from a mixture of several Gaussian distributions.\n",
    "- **Algorithm**: Uses the Expectation-Maximization (EM) algorithm to estimate model parameters.\n",
    "\n",
    "#### 6. Spectral Clustering\n",
    "\n",
    "- **Description**: Uses the eigenvalues of the similarity matrix to reduce dimensionality before clustering in a lower-dimensional space.\n",
    "- **Algorithm**:\n",
    "  1. Build a similarity graph.\n",
    "  2. Compute the Laplacian of this graph.\n",
    "  3. Extract eigenvectors and use them as features.\n",
    "  4. Use K-means or another method on these new features.\n",
    "\n",
    "#### 7. Affinity Propagation\n",
    "\n",
    "- **Description**: Works by passing messages between data points to determine clusters.\n",
    "- **Algorithm**:\n",
    "  1. Send responsibility and availability messages between points.\n",
    "  2. Iteratively update messages and decide cluster exemplars.\n",
    "\n",
    "#### 8. OPTICS (Ordering Points To Identify Clustering Structure)\n",
    "\n",
    "- **Description**: Similar to DBSCAN but can handle clusters of varying densities.\n",
    "- **Algorithm**: Builds a reachability plot and extracts clusters from it.\n",
    "\n",
    "#### 9. BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)\n",
    "\n",
    "- **Description**: Designed for very large datasets. Builds a tree structure during clustering.\n",
    "- **Algorithm**:\n",
    "  1. Incrementally construct a CF (Clustering Feature) tree.\n",
    "  2. Use the CF tree for clustering.\n",
    "\n",
    "#### 10. Agglomerative Clustering\n",
    "\n",
    "- **Description**: Similar to hierarchical clustering but typically stops when it reaches a desired number of clusters.\n",
    "- **Algorithm**:\n",
    "  1. Start with each data point as a single cluster.\n",
    "  2. Recursively merge the closest pair of clusters.\n",
    "  3. Stop when the desired number of clusters is reached.\n",
    "\n",
    "#### 11. CURE (Clustering Using Representatives)\n",
    "\n",
    "- **Description**: A robust method for clustering with outliers by representing clusters using multiple scattered points.\n",
    "- **Algorithm**:\n",
    "  1. Select a set of representative points for each cluster.\n",
    "  2. Shrink the representatives towards the center of the cluster.\n",
    "  3. Assign points to the closest representative's cluster.\n",
    "\n",
    "#### 12. K-Medoids or PAM (Partitioning Around Medoids)\n",
    "\n",
    "- **Description**: Similar to K-means but uses actual data points as cluster centers (medoids) to reduce the influence of outliers.\n",
    "- **Algorithm**:\n",
    "  1. Randomly select \\(K\\) data points as initial medoids.\n",
    "  2. Assign each data point to the nearest medoid.\n",
    "  3. Recompute medoids.\n",
    "  4. Repeat steps 2-3 until convergence.\n",
    "\n",
    "#### 13. COBWEB\n",
    "\n",
    "- **Description**: A hierarchical clustering method for categorical data.\n",
    "- **Algorithm**:\n",
    "  1. Construct a tree structure.\n",
    "  2. Place and categorize instances based on maximizing the category utility.\n",
    "\n",
    "#### 14. Fuzzy C-Means\n",
    "\n",
    "- **Description**: A soft clustering method where each data point has a degree of belonging to clusters, rather than belonging strictly to one cluster.\n",
    "- **Algorithm**:\n",
    "  1. Assign each data point a weight for each cluster.\n",
    "  2. Compute cluster centers based on weights.\n",
    "  3. Update weights for each data point based on distances to cluster centers.\n",
    "  4. Repeat steps 2-3 until convergence.\n",
    "\n",
    "#### 15. ROCK (RObust Clustering using linKs)\n",
    "\n",
    "- **Description**: Designed for categorical data and uses links (similar pairs) for clustering.\n",
    "- **Algorithm**:\n",
    "  1. Count the number of common neighbors between points to form links.\n",
    "  2. Form clusters based on these links.\n",
    "\n",
    "#### 16. CLIQUE (CLustering In QUEst)\n",
    "\n",
    "- **Description**: A grid-based clustering algorithm designed to find dense clusters in subspaces of any dimensionality.\n",
    "- **Algorithm**:\n",
    "  1. Partition each dimension into non-overlapping intervals.\n",
    "  2. Identify dense units in each dimension.\n",
    "  3. Form clusters based on the connectivity of dense units.\n",
    "\n",
    "#### 17. SNN (Shared Nearest Neighbor)\n",
    "\n",
    "- **Description**: Considers two objects similar if they have many neighbors in common.\n",
    "- **Algorithm**:\n",
    "  1. Compute the k-nearest neighbors for each point.\n",
    "  2. Assign similarity based on shared neighbors.\n",
    "  3. Cluster based on these similarities.\n",
    "\n",
    "#### 18. STREAM (Spatio-TEmporal Real-time Algorithm for clustering Moving objects)\n",
    "\n",
    "- **Description**: Designed for real-time clustering of moving objects.\n",
    "- **Algorithm**:\n",
    "  1. Uses micro-clusters to summarize the current state.\n",
    "  2. Periodically merges these micro-clusters to form final clusters.\n",
    "\n",
    "#### 19. SUBCLU (SUBspace CLUstering)\n",
    "\n",
    "- **Description**: Discovers clusters in arbitrary subspaces of a dataset.\n",
    "- **Algorithm**:\n",
    "  1. Uses a bottom-up approach.\n",
    "  2. Discovers dense regions in each dimension and merges them to identify clusters.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The choice of clustering method often depends on the dataset's size, dimensionality, and nature, the desired shape and structure of the clusters, and any underlying assumptions about the data. Pre-processing, like normalization or standardization, and the right distance or similarity measure, can also play crucial roles in clustering outcomes.\n",
    "\n",
    "The variety of clustering methods available offers flexibility in handling different types of datasets and various challenges like noise, outliers, and non-globular cluster shapes. The choice of a method should be guided by the nature of the data and the specific requirements of the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2d5ec",
   "metadata": {},
   "source": [
    "#### Exercise 1 Understanding K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f982fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "import warnings \n",
    "  \n",
    "# Settings the warnings to be ignored \n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fca51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Defining the number of clusters\n",
    "k = 4\n",
    "\n",
    "# Applying KMeans clustering\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualizing the clusters and centroids\n",
    "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s=50, c='red', label='Cluster 1')\n",
    "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s=50, c='blue', label='Cluster 2')\n",
    "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s=50, c='green', label='Cluster 3')\n",
    "plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s=50, c='yellow', label='Cluster 4')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='black', marker='X', label='Centroids')\n",
    "plt.legend()\n",
    "plt.title(\"Clusters of data points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b6ae4",
   "metadata": {},
   "source": [
    "#### Exercise 2 Understanding Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53acd877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456299af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "# Computing the distance matrix\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\n",
    "\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Euclidean Distances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0401e994",
   "metadata": {},
   "source": [
    "#### Exercise 3 Understanding DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c062a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset with clusters of different shapes\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, _ = make_moons(n_samples=300, noise=0.05, random_state=0)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a987c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Applying DBSCAN\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
    "clusters = dbscan.fit_predict(X)\n",
    "\n",
    "# Visualizing the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', s=50)\n",
    "plt.title('Clusters identified by DBSCAN')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd771026",
   "metadata": {},
   "source": [
    "#### Exercise 4 Understanding Mean Shift Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset with blobs\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=5, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "# Applying Mean Shift\n",
    "mean_shift = MeanShift(bandwidth=1.5)\n",
    "clusters = mean_shift.fit_predict(X)\n",
    "\n",
    "# Visualizing the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', s=50)\n",
    "plt.title('Clusters identified by Mean Shift')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e1f43",
   "metadata": {},
   "source": [
    "#### Exercise 5 Understanding Gaussian Mixture Models (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f694509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset with blobs\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=5, cluster_std=0.60, random_state=42)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ae372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Applying GMM\n",
    "gmm = GaussianMixture(n_components=5)\n",
    "gmm.fit(X)\n",
    "labels = gmm.predict(X)\n",
    "\n",
    "# Visualizing the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.title('Clusters identified by GMM')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5153176",
   "metadata": {},
   "source": [
    "#### Exercise 6 Understanding Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset with moons\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, _ = make_moons(200, noise=.05, random_state=42)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Applying Spectral Clustering\n",
    "sc = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', assign_labels='kmeans')\n",
    "labels = sc.fit_predict(X)\n",
    "\n",
    "# Visualizing the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.title('Clusters identified by Spectral Clustering')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c44066",
   "metadata": {},
   "source": [
    "#### Exercise 7 Understanding Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset with blobs\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, _ = make_blobs(300, centers=5, cluster_std=0.60, random_state=42)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad8bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# Applying Affinity Propagation\n",
    "af = AffinityPropagation(preference=-50).fit(X)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "labels = af.labels_\n",
    "\n",
    "# Visualizing the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.scatter(X[cluster_centers_indices, 0], X[cluster_centers_indices, 1], s=200, c='red', marker='X', label='Exemplars')\n",
    "plt.title('Clusters identified by Affinity Propagation')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1406038e",
   "metadata": {},
   "source": [
    "#### Exercise 8 Understanding OPTICS Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3686924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset with blobs and noise\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "# Applying OPTICS\n",
    "optics = OPTICS(min_samples=10, xi=0.05, min_cluster_size=0.05)\n",
    "labels = optics.fit_predict(X)\n",
    "\n",
    "# Visualizing the reachability plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.title('Reachability plot')\n",
    "plt.plot(optics.reachability_[optics.ordering_])\n",
    "plt.xlabel('Points')\n",
    "plt.ylabel('Reachability Distance')\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.title('Clusters identified by OPTICS')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c287b",
   "metadata": {},
   "source": [
    "#### Exercise 9 Understanding BIRCH Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5277da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset with blobs\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, _ = make_blobs(n_samples=500, centers=5, cluster_std=0.6, random_state=42)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54174b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "\n",
    "# Applying BIRCH\n",
    "birch = Birch(n_clusters=5, threshold=0.5, branching_factor=50)\n",
    "labels = birch.fit_predict(X)\n",
    "\n",
    "# Visualizing the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.title('Clusters identified by BIRCH')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f28aa",
   "metadata": {},
   "source": [
    "#### Exercise 10 Understanding Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b589a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset with blobs\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, _ = make_blobs(n_samples=500, centers=5, cluster_std=0.6, random_state=42)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c35011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Applying Agglomerative Clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=5, linkage='ward')\n",
    "labels = agg_clustering.fit_predict(X)\n",
    "\n",
    "# Visualizing the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.title('Clusters identified by Agglomerative Clustering')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c8b63",
   "metadata": {},
   "source": [
    "#### Exercise 11 Understanding Clustering Using Representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset with blobs\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, _ = make_blobs(n_samples=500, centers=5, cluster_std=0.6, random_state=42)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4293104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "# Randomly select representatives\n",
    "rng = np.random.RandomState(42)\n",
    "i = rng.permutation(X.shape[0])[:5]\n",
    "representatives = X[i]\n",
    "\n",
    "while True:\n",
    "    # Assign labels based on closest representative\n",
    "    labels = pairwise_distances_argmin(X, representatives)\n",
    "    \n",
    "    # Find new representatives from data points\n",
    "    new_representatives = np.array([X[labels == i].mean(0) for i in range(5)])\n",
    "    \n",
    "    # Check for convergence\n",
    "    if np.all(representatives == new_representatives):\n",
    "        break\n",
    "    representatives = new_representatives\n",
    "\n",
    "# Visualizing the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.scatter(representatives[:, 0], representatives[:, 1], c='black', s=200, alpha=0.75)\n",
    "plt.title('Clusters identified by Clustering Using Representatives')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad4ea6",
   "metadata": {},
   "source": [
    "####  Exercise 12 Understanding Partitioning Around Medoids (PAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3121d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset with blobs\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, _ = make_blobs(n_samples=500, centers=5, cluster_std=0.6, random_state=42)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756abb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "# Applying PAM clustering\n",
    "kmedoids = KMedoids(n_clusters=5, random_state=42).fit(X)\n",
    "\n",
    "# Getting labels and medoids\n",
    "labels = kmedoids.labels_\n",
    "medoids = kmedoids.cluster_centers_\n",
    "\n",
    "# Visualizing the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.scatter(medoids[:, 0], medoids[:, 1], c='red', s=200, alpha=0.75, marker='X')\n",
    "plt.title('Clusters identified by PAM')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa4953",
   "metadata": {},
   "source": [
    "####  Exercise 13 Understanding Fuzzy C-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209be4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset with blobs\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, _ = make_blobs(n_samples=500, centers=5, cluster_std=0.6, random_state=42)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d8fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fuzzy-c-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41986ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcmeans import FCM\n",
    "\n",
    "# Applying FCM clustering\n",
    "fcm = FCM(n_clusters=5)\n",
    "fcm.fit(X)\n",
    "\n",
    "# Getting labels and centers\n",
    "labels = fcm.predict(X)\n",
    "centers = fcm.centers\n",
    "\n",
    "# Visualizing the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X')\n",
    "plt.title('Clusters identified by Fuzzy C-Means')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc0966",
   "metadata": {},
   "source": [
    "####  Exercise 14 Case Study: K-Means Clustering on the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4545d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading the Iris dataset\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Displaying the first few rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af9b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the elbow method to find the optimal number of clusters\n",
    "\n",
    "wcss = []  # Within-cluster sum of squares\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "    kmeans.fit(df)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the elbow method graph\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1, 11), wcss, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e8e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying k-means clustering with 3 clusters\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
    "y_kmeans = kmeans.fit_predict(df)\n",
    "\n",
    "# Visualizing the clusters on the first two columns\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(df.iloc[y_kmeans == 0, 0], df.iloc[y_kmeans == 0, 1], s=50, c='red', label='Cluster 1')\n",
    "plt.scatter(df.iloc[y_kmeans == 1, 0], df.iloc[y_kmeans == 1, 1], s=50, c='blue', label='Cluster 2')\n",
    "plt.scatter(df.iloc[y_kmeans == 2, 0], df.iloc[y_kmeans == 2, 1], s=50, c='green', label='Cluster 3')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label='Centroids')\n",
    "plt.title('Clusters of Iris Flowers')\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d5564",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f5911",
   "metadata": {},
   "source": [
    "#### Revised Date: November 11, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c487caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
