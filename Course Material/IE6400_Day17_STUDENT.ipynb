{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xz07SAS2mjcZ"
   },
   "source": [
    "# IE6400 Foundations of Data Analytics Engineering\n",
    "# Fall 2023 \n",
    "### Module 3: Linear Algebra Part-2\n",
    "#### - STUDENT VERSION -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices\n",
    "\n",
    "\"Matrices\" (singular: matrix) refer to rectangular arrays of numbers, symbols, or expressions, arranged in rows and columns. They are a fundamental concept in linear algebra and have various applications in pure mathematics, physics, computer science, economics, and other fields.\n",
    "\n",
    "For example, consider the matrix A:\n",
    "\n",
    "> $\n",
    "A = \\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "This matrix has 2 rows and 2 columns, making it a 2x2 matrix. The entry in the first row and first column is \\(a\\), the entry in the first row and second column is \\(b\\), and so on.\n",
    "\n",
    "#### Types of Matrices\n",
    "\n",
    "There are several types of matrices, including:\n",
    "- **Row matrix**: Only one row.\n",
    "- **Column matrix**: Only one column.\n",
    "- **Square matrix**: Same number of rows and columns.\n",
    "- **Diagonal matrix**: All non-diagonal elements are zero.\n",
    "- **Identity matrix**: A square matrix where diagonal elements are 1 and non-diagonal elements are 0.\n",
    "- **Zero matrix**: All elements are zero.\n",
    "- **Symmetric matrix**: It remains unchanged upon taking its transpose.\n",
    "- **Skew-symmetric matrix**: Its negative is its transpose.\n",
    "- **Orthogonal matrix**: Its transpose is its inverse.\n",
    "\n",
    "#### Matrix Operations\n",
    "\n",
    "Several operations can be performed with matrices, including:\n",
    "- **Addition**: You can add two matrices of the same dimension by adding corresponding elements.\n",
    "- **Scalar Multiplication**: Multiply each element of the matrix by a scalar (a single number).\n",
    "- **Matrix Multiplication**: Multiplying two matrices involves taking the dot product of rows of the first matrix with columns of the second.\n",
    "- **Transpose**: Reflect the matrix over its main diagonal.\n",
    "- **Determinant**: A scalar value derived from a square matrix.\n",
    "- **Inverse**: If it exists, the inverse of a matrix is such that when it's multiplied with the original matrix, the result is the identity matrix.\n",
    "\n",
    "#### Applications of Matrices\n",
    "\n",
    "Matrices are used in various fields and applications, including:\n",
    "- **Linear Transformations**: Representing transformations in geometry.\n",
    "- **Solving Systems of Linear Equations**: Using techniques like Gauss-Jordan elimination or Cramer's rule.\n",
    "- **Computer Graphics**: For transformations like rotation, scaling, and translation.\n",
    "- **Quantum Mechanics**: Representing quantum states and operations.\n",
    "- **Economics**: Input-output analysis.\n",
    "- **Data Analysis**: In areas such as Principal Component Analysis (PCA).\n",
    "\n",
    "Understanding the properties and operations of matrices is foundational in linear algebra, and linear algebra itself is crucial for many advanced topics in mathematics and its applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNobeNgHNToe"
   },
   "source": [
    "### Types of Matrices\n",
    "\n",
    "A lot of linear algebra is concerned with operations on vectors and matrices, and there are many\n",
    "different types of matrices. There are a few types of matrices that you may encounter again and\n",
    "again when getting started in linear algebra, particularity the parts of linear algebra relevant to\n",
    "machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dUEJUDkNqxS"
   },
   "source": [
    "#### Square Matrix\n",
    "A square matrix is a matrix where the number of rows $(n)$ is equivalent to the number of\n",
    "columns $(m)$.\n",
    "\n",
    "> $n=m$\n",
    "\n",
    "The size of the matrix is called the order, so an order 4 square\n",
    "matrix is 4 × 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symmetric Matrix\n",
    "\n",
    "A symmetric matrix is a type of square matrix where the top-right triangle is the same as the\n",
    "bottom-left triangle.\n",
    "\n",
    "To be symmetric, the axis of symmetry is always the main diagonal of the matrix, from the\n",
    "top left to the bottom right.\n",
    "\n",
    "A symmetric matrix is always square and equal to its own transpose. The transpose is an\n",
    "operation that flips the number of rows and columns. It is explained in more detail in the next\n",
    "lesson.\n",
    "\n",
    "> $M = M^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 Understanding Square Matrices using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a random 4x4 square matrix\n",
    "np.random.seed(0)\n",
    "square_matrix = np.random.randint(1, 10, size=(4, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the square matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(square_matrix, annot=True, cmap='viridis', cbar=False)\n",
    "plt.title('Square Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the matrix is symmetric\n",
    "is_symmetric = np.array_equal(square_matrix, square_matrix.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_symmetric:\n",
    "    print(\"The matrix is symmetric.\")\n",
    "else:\n",
    "    print(\"The matrix is not symmetric.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cffxyGa8TmyH"
   },
   "source": [
    "#### Triangular Matrix\n",
    "\n",
    "A triangular matrix is a type of square matrix that has all values in the upper-right or lower-left\n",
    "of the matrix with the remaining elements filled with zero values. A triangular matrix with\n",
    "values only above the main diagonal is called an upper triangular matrix. Whereas, a triangular\n",
    "matrix with values only below the main diagonal is called a lower triangular matrix. Below is\n",
    "an example of a 3 × 3 upper triangular matrix.\n",
    "\n",
    "> $a = \\begin{bmatrix}\n",
    "  1 & 2 & 3  \\\\\n",
    "  0 & 2 & 3\\\\\n",
    "  0 & 0 & 3\n",
    " \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2 Understanding Triangular Matrices using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a random 5x5 matrix\n",
    "np.random.seed(0)\n",
    "matrix_A = np.random.randint(1, 10, size=(5, 5))\n",
    "\n",
    "# Extracting the upper and lower triangular matrices\n",
    "upper_triangular = np.triu(matrix_A)\n",
    "lower_triangular = np.tril(matrix_A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the matrices as heatmaps\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sns.heatmap(matrix_A, annot=True, cmap='viridis', cbar=False, ax=ax[0])\n",
    "ax[0].set_title('Original Matrix')\n",
    "\n",
    "sns.heatmap(upper_triangular, annot=True, cmap='viridis', cbar=False, ax=ax[1])\n",
    "ax[1].set_title('Upper Triangular Matrix')\n",
    "\n",
    "sns.heatmap(lower_triangular, annot=True, cmap='viridis', cbar=False, ax=ax[2])\n",
    "ax[2].set_title('Lower Triangular Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Kgt1Dt9WrK5"
   },
   "source": [
    "#### Diagonal Matrix\n",
    "\n",
    "A diagonal matrix is one where values outside of the main diagonal have a zero value, where the\n",
    "main diagonal is taken from the top left of the matrix to the bottom right. A diagonal matrix\n",
    "is often denoted with the variable D and may be represented as a full matrix or as a vector of\n",
    "values on the main diagonal. Below is an example of a 3 × 3 square diagonal matrix.\n",
    "\n",
    "> $a = \\begin{bmatrix}\n",
    "  1 & 0 & 0  \\\\\n",
    "  0 & 1 & 0\\\\\n",
    "  0 & 0 & 1\n",
    " \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3 Understanding Diagonal Matrices using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a random 5x5 diagonal matrix\n",
    "np.random.seed(0)\n",
    "diagonal_values = np.random.randint(1, 10, size=5)\n",
    "diagonal_matrix = np.diag(diagonal_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the diagonal matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(diagonal_matrix, annot=True, cmap='viridis', cbar=False)\n",
    "plt.title('Diagonal Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5I_faw9YMY-"
   },
   "source": [
    "#### Identity Matrix\n",
    "\n",
    "An identity matrix is a square matrix that does not change a vector when multiplied. The\n",
    "values of an identity matrix are known. All of the scalar values along the main diagonal (top-left\n",
    "to bottom-right) have the value one, while all other values are zero.\n",
    "\n",
    "> $a = \\begin{bmatrix}\n",
    "  1 & 0 & 0  \\\\\n",
    "  0 & 1 & 0\\\\\n",
    "  0 & 0 & 1\n",
    " \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4 Understanding Identity Matrices using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a 5x5 identity matrix\n",
    "identity_matrix = np.eye(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the identity matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(identity_matrix, annot=True, cmap='viridis', cbar=False)\n",
    "plt.title('Identity Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEirqy_CZhkp"
   },
   "source": [
    "#### Orthogonal Matrix\n",
    "\n",
    "Two vectors are orthogonal when their dot product equals zero. The length of each vector is 1\n",
    "then the vectors are called orthonormal because they are both orthogonal and normalized.\n",
    "\n",
    "> $v.w^t = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5 Understanding Orthogonal Matrices using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a 2x2 orthogonal matrix\n",
    "theta = np.pi / 4  # 45 degree rotation\n",
    "orthogonal_matrix = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                              [np.sin(theta), np.cos(theta)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying orthogonality\n",
    "resultant_matrix = np.dot(orthogonal_matrix, orthogonal_matrix.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the orthogonal matrix and resultant matrix as heatmaps\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.heatmap(orthogonal_matrix, annot=True, cmap='viridis', cbar=False, ax=ax1)\n",
    "ax1.set_title('Orthogonal Matrix')\n",
    "\n",
    "sns.heatmap(resultant_matrix, annot=True, cmap='viridis', cbar=False, ax=ax2)\n",
    "ax2.set_title('Resultant Matrix (Orthogonal Matrix * Transpose)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTGyNSZopZxP"
   },
   "source": [
    "### Matrix Operations\n",
    "\n",
    "Matrix operations are used in the description of many machine learning algorithms. Some\n",
    "operations can be used directly to solve key equations, whereas others provide useful shorthand\n",
    "or foundation in the description and the use of more complex matrix operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEK-3iJOpqNW"
   },
   "source": [
    "#### Transpose\n",
    "A defined matrix can be transposed, which creates a new matrix with the number of columns\n",
    "and rows flipped. This is denoted by the superscript $T$ next to the matrix $A^T$.\n",
    "\n",
    "> $C = A^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH-24b83IiHd"
   },
   "source": [
    "#### Exercise 6 Understanding the Transpose of a Matrix using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a 3x2 matrix\n",
    "matrix = np.array([[1, 2],\n",
    "                  [3, 4],\n",
    "                  [5, 6]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the transpose of the matrix\n",
    "transpose_matrix = matrix.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the matrix and its transpose as heatmaps\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.heatmap(matrix, annot=True, cmap='viridis', cbar=False, ax=ax1)\n",
    "ax1.set_title('Original Matrix')\n",
    "\n",
    "sns.heatmap(transpose_matrix, annot=True, cmap='viridis', cbar=False, ax=ax2)\n",
    "ax2.set_title('Transpose Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBpgQWHBIqPA"
   },
   "source": [
    "#### Exercise 7 Transposing a Matrix using Nested List Comprehension in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a 3x2 matrix using nested lists\n",
    "matrix = [[1, 2],\n",
    "          [3, 4],\n",
    "          [5, 6]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposing the matrix using nested list comprehension\n",
    "transpose_matrix = [[row[i] for row in matrix] for i in range(len(matrix[0]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the original and transposed matrices\n",
    "print(\"Original Matrix:\")\n",
    "for row in matrix:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nTransposed Matrix:\")\n",
    "for row in transpose_matrix:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnTMg2J1sKxI"
   },
   "source": [
    "#### Inverse\n",
    "\n",
    "Matrix inversion is a process that finds another matrix that when multiplied with the matrix, results in an identity matrix. Given a matrix $A$, find matrix $B$, such that $AB = I^n$\n",
    " or $BA = I^n$\n",
    "\n",
    "> $AB = BA = I^n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1rWuieyL7u4"
   },
   "source": [
    "#### Exercise 8 Understanding Matrix Inversion in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generating a random 3x3 matrix\n",
    "matrix = np.random.rand(3, 3)\n",
    "print(\"Original Matrix:\")\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diXKBXXbJ3XS"
   },
   "outputs": [],
   "source": [
    "# Computing the inverse of the matrix\n",
    "inverse_matrix = np.linalg.inv(matrix)\n",
    "print(\"Inverse Matrix:\")\n",
    "print(inverse_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6JoQoE4IJ8SI",
    "outputId": "df8de3ea-3f94-4c97-f7df-d730a843af76"
   },
   "outputs": [],
   "source": [
    "# Multiplying the matrix with its inverse\n",
    "product = np.dot(matrix, inverse_matrix)\n",
    "print(\"Product of Matrix and its Inverse:\")\n",
    "print(product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRI3_6QavOZv"
   },
   "source": [
    "#### Trace\n",
    "\n",
    "A trace of a square matrix is the sum of the values on the main diagonal of the matrix (top-left\n",
    "to bottom-right).\n",
    "\n",
    "The trace operator gives the sum of all of the diagonal entries of a matrix.\n",
    "\n",
    "The operation of calculating a trace on a square matrix is described using the notation $tr(A)$\n",
    "where A is the square matrix on which the operation is being performed.\n",
    "\n",
    "The trace is calculated as the sum of the diagonal values; for example, in the case of a 3 × 3 matrix:\n",
    "$tr(A) = a_1,_1+a_2,_2+a_3,_3$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmQ3E7CDN9Qc"
   },
   "source": [
    "#### Exercise 9 Understanding the Trace of a Matrix in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generating a random 4x4 matrix\n",
    "matrix = np.random.rand(4, 4)\n",
    "print(\"Original Matrix:\")\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the trace of the matrix\n",
    "trace_value = np.trace(matrix)\n",
    "print(f\"Trace of the Matrix: {trace_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_ZkUCWcNyOL",
    "outputId": "c75e89a6-01f6-4362-dfee-c9c47ec3b394"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(matrix, cmap='viridis')\n",
    "plt.colorbar()\n",
    "# Highlight the diagonal elements\n",
    "plt.scatter([0, 1, 2, 3], [0, 1, 2, 3], color='red', marker='x')\n",
    "plt.title(\"Matrix with Diagonal Elements Highlighted\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_wJhQPPwIz_"
   },
   "source": [
    "#### Determinant\n",
    "\n",
    "The determinant of a square matrix is a scalar representation of the volume of the matrix.\n",
    "\n",
    "It is denoted by the $det(A)$ notation or $|A|$, where A is the matrix on which we are calculating\n",
    "the determinant.\n",
    "\n",
    "> $det(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHYxl5UuPCn_"
   },
   "source": [
    "#### Exercise 10 Understanding the Determinant of a Matrix in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdIWm1uWPMnH",
    "outputId": "a5cb0e55-a316-4e6c-ab9b-64e2289823a4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generating a random 3x3 matrix\n",
    "matrix = np.random.rand(3, 3)\n",
    "print(\"Original Matrix:\")\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the determinant of the matrix\n",
    "determinant_value = np.linalg.det(matrix)\n",
    "print(f\"Determinant of the Matrix: {determinant_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualizing the matrix using a heatmap\n",
    "sns.heatmap(matrix, cmap='viridis', annot=True, cbar=False)\n",
    "plt.title(\"Matrix Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMHkZ64Iw9vV"
   },
   "source": [
    "#### Rank\n",
    "\n",
    "The rank of a matrix is the estimate of the number of linearly independent rows or columns in\n",
    "a matrix. The rank of a matrix M is often denoted as the function $rank().$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtK9EP0JPugi"
   },
   "source": [
    "#### Exercise 11 Understanding the Rank of a Matrix in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hk6L-nLrQJxW",
    "outputId": "03ca0376-de4e-4ce8-8232-3311cd2de42e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generating a random 4x4 matrix\n",
    "matrix = np.random.rand(4, 4)\n",
    "print(\"Original Matrix:\")\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the rank of the matrix\n",
    "rank_value = np.linalg.matrix_rank(matrix)\n",
    "print(f\"Rank of the Matrix: {rank_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'matrix' is a 2D numpy array or similar data structure\n",
    "sns.heatmap(matrix, cmap='viridis', annot=True)\n",
    "plt.title(\"Matrix Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DohLP6s7nnb2"
   },
   "source": [
    "#### Understanding Eigenvectors and Eigenvalues with Python\n",
    "\n",
    "> **Eigenvalues :** are scalars $(numbers)$ associated with a square matrix. Given a square matrix $A$, an eigenvalue $(λ) $of $A$ is a scalar that satisfies the following equation:\n",
    ">\n",
    "> $A * v = λ * v$\n",
    "\n",
    "> Where:\n",
    ">\n",
    ">  1.   $A$ is the square matrix under consideration.\n",
    ">  2.    $v$ is a non-zero vector called the eigenvector associated with the eigenvalue $λ.$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nADADRrqIRN"
   },
   "source": [
    "> **Eigenvectors:** Eigenvectors are non-zero vectors that correspond to eigenvalues. For each eigenvalue $(λ)$ of a square matrix $A$, there exists an associated eigenvector $v$ such that when the matrix $A$ is multiplied by this eigenvector, the result is a scaled version of the eigenvector:\n",
    ">\n",
    ">$A * v = λ * v$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQjQB4ntRRzx"
   },
   "source": [
    "#### Exercise 12 Understanding Eigenvectors and Eigenvalues in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzqDbCNPstdi",
    "outputId": "395e5923-fe32-4738-c74c-583f368ad58d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generating a random 3x3 matrix\n",
    "matrix = np.random.rand(3, 3)\n",
    "print(\"Original Matrix:\")\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPWPwaSuRAm0"
   },
   "outputs": [],
   "source": [
    "# Computing the eigenvectors and eigenvalues\n",
    "eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "print(\"Eigenvectors:\")\n",
    "print(eigenvectors)\n",
    "print(\"\\nEigenvalues:\")\n",
    "print(eigenvalues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'matrix' is a 2D numpy array or similar data structure\n",
    "sns.heatmap(matrix, cmap='viridis', annot=True, cbar=False)\n",
    "plt.title(\"Matrix Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the real part of the eigenvectors\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Origin for the eigenvectors\n",
    "origin = [0, 0, 0]\n",
    "\n",
    "# Plotting the real part of the eigenvectors\n",
    "for i in range(3):\n",
    "    ax.quiver(origin, origin, np.real(eigenvectors[0, i]), np.real(eigenvectors[1, i]), angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "ax.set_xlim([-1, 1])\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.set_aspect('equal')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Visualizing the real part of the eigenvectors in a 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Origin for the eigenvectors\n",
    "origin = np.array([0, 0, 0])\n",
    "\n",
    "# Plotting the real part of the eigenvectors\n",
    "for i in range(len(eigenvectors)):\n",
    "    ax.quiver(*origin, np.real(eigenvectors[0, i]), np.real(eigenvectors[1, i]), np.real(eigenvectors[2, i]))\n",
    "\n",
    "ax.set_xlim([-1, 1])\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.set_zlim([-1, 1])\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title(\"Eigenvectors\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fI4KsTy5Te62"
   },
   "source": [
    "#### Exercise 13  Compound Interest Comparison Using Matrix Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interest rates\n",
    "r1 = 0.05\n",
    "r2 = 0.03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the growth matrix\n",
    "growth_matrix = np.array([[1 + r1, 1 + r2]])\n",
    "for i in range(1, 10):\n",
    "    growth_matrix = np.vstack([growth_matrix, [growth_matrix[i-1, 0]*(1+r1), growth_matrix[i-1, 1]*(1+r2)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_investment = np.array([[10000, 10000]])\n",
    "\n",
    "# Calculate the amounts for both accounts over 10 years\n",
    "amounts = initial_investment * growth_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the growth\n",
    "plt.plot(amounts[:, 0], label=\"Account 1 (5% interest)\")\n",
    "plt.plot(amounts[:, 1], label=\"Account 2 (3% interest)\")\n",
    "plt.xlabel(\"Years\")\n",
    "plt.ylabel(\"Amount ($)\")\n",
    "plt.title(\"Growth of Investment Over 10 Years\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK2uR0DCl51W"
   },
   "source": [
    "#### Matrix Decomposition\n",
    "\n",
    "Many complex matrix operations cannot be solved efficiently or with stability using the limited\n",
    "precision of computers.\n",
    "\n",
    "Matrix decompositions are methods that reduce a matrix into constituent\n",
    "parts that make it easier to calculate more complex matrix operations.\n",
    "\n",
    "Matrix decomposition methods, also called matrix factorization methods,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPFBzZfP0imn"
   },
   "source": [
    "#####  LU Decomposition\n",
    "\n",
    "> The $LU$ decomposition is for square matrices and decomposes a matrix into L and U components.\n",
    ">\n",
    ">$A = L.U$\n",
    ">\n",
    ">Where $A$ is the square matrix that we wish to decompose, $L$ is the lower triangle matrix\n",
    "and $U$ is the upper triangle matrix.\n",
    ">\n",
    ">The LU decomposition is found using an iterative numerical process and can fail for those\n",
    "matrices that cannot be decomposed or decomposed easily. A variation of this decomposition\n",
    "that is numerically more stable to solve in practice is called the LUP decomposition, or the LU\n",
    "decomposition with partial pivoting.\n",
    ">\n",
    ">$A = L.U.P$\n",
    ">\n",
    ">The $LU$ decomposition is calculated, then the original matrix is reconstructed\n",
    "from the components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pVm-Ie1UwGR"
   },
   "source": [
    "#### Exercise 14 LU Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import lu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the matrix A\n",
    "A = np.array([[6, 2, 1],\n",
    "              [2, 3, 1],\n",
    "              [1, -1, 2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform LU Decomposition\n",
    "P, L, U = lu(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"P matrix:\")\n",
    "print(P)\n",
    "print(\"\\nL matrix:\")\n",
    "print(L)\n",
    "print(\"\\nU matrix:\")\n",
    "print(U)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'A', 'L', 'U' are 2D numpy arrays representing matrices\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "sns.heatmap(A, ax=axs[0], cmap=\"viridis\", annot=True, cbar=False)\n",
    "axs[0].set_title(\"Matrix A\")\n",
    "\n",
    "sns.heatmap(L, ax=axs[1], cmap=\"viridis\", annot=True, cbar=False)\n",
    "axs[1].set_title(\"Lower Triangular Matrix L\")\n",
    "\n",
    "sns.heatmap(U, ax=axs[2], cmap=\"viridis\", annot=True, cbar=False)\n",
    "axs[2].set_title(\"Upper Triangular Matrix U\")\n",
    "\n",
    "sns.heatmap(np.dot(L, U), ax=axs[3], cmap=\"viridis\", annot=True, cbar=False)\n",
    "axs[3].set_title(\"L x U\")\n",
    "\n",
    "# Create a colorbar for the entire figure\n",
    "cbar_ax = fig.add_axes([0.93, 0.15, 0.02, 0.7])  # x, y, width, height\n",
    "fig.colorbar(axs[3].collections[0], cax=cbar_ax)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPXSCMhq2UcC"
   },
   "source": [
    "####  QR Decomposition\n",
    "\n",
    "The QR decomposition is for $n × m$ matrices (not limited to square matrices) and decomposes\n",
    "a matrix into Q and R components.\n",
    "\n",
    "QR decomposition decomposes a given matrix A into the product of an orthogonal matrix (Q) and an upper triangular matrix (R), i.e., A = QR.\n",
    "> $A=Q.R$\n",
    "\n",
    "Where $A$ is the matrix that we wish to decompose, $Q$ a matrix with the size $m × m,$ and $R$ is\n",
    "an upper triangle matrix with the size $m × n.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxPomNxnVIQ8"
   },
   "source": [
    "#### Exercise 15 QR Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the matrix M\n",
    "M = np.array([[12, -51, 4],\n",
    "              [6, 167, -68],\n",
    "              [-4, 24, -41]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform QR Decomposition\n",
    "Q, R = np.linalg.qr(M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Q matrix:\")\n",
    "print(Q)\n",
    "print(\"\\nR matrix:\")\n",
    "print(R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'M', 'Q', 'R' are 2D numpy arrays representing matrices\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "sns.heatmap(M, ax=ax[0], cmap=\"viridis\", annot=True, cbar=False)\n",
    "ax[0].set_title(\"Matrix M\")\n",
    "\n",
    "sns.heatmap(Q, ax=ax[1], cmap=\"viridis\", annot=True, cbar=False)\n",
    "ax[1].set_title(\"Orthogonal Matrix Q\")\n",
    "\n",
    "sns.heatmap(R, ax=ax[2], cmap=\"viridis\", annot=True, cbar=False)\n",
    "ax[2].set_title(\"Upper Triangular Matrix R\")\n",
    "\n",
    "sns.heatmap(np.dot(Q, R), ax=ax[3], cmap=\"viridis\", annot=True, cbar=False)\n",
    "ax[3].set_title(\"Q x R\")\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-LgDUlyA2LZ"
   },
   "source": [
    "## Application of Matrix Decomposition\n",
    "**1. Solving Linear Systems:**\n",
    "\n",
    "Matrix decomposition methods, such as LU decomposition, are used to solve systems of linear equations efficiently. By decomposing the coefficient matrix, solving for the unknown variables becomes more manageable.\n",
    "\n",
    " **2. Principal Component Analysis (PCA):**\n",
    "\n",
    "PCA involves eigenvalue decomposition or singular value decomposition (SVD) to identify the principal components in high-dimensional data. It is a fundamental technique in dimensionality reduction and data visualization.\n",
    "\n",
    "**3. Matrix Approximations:**\n",
    "\n",
    "Matrix factorization techniques like SVD, QR decomposition, and NMF are used to approximate large matrices, reducing their dimensionality while preserving essential information. These approximations are used in various data analysis tasks.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFUqLL64cLy6"
   },
   "source": [
    "#### Singular Value Decomposition (SVD)\n",
    "\n",
    "> The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for\n",
    "reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations\n",
    "simpler.\n",
    ">\n",
    "> $A = U · Σ · V^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdEa96gyVVKb"
   },
   "source": [
    "#### Exercise 16 Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the matrix A\n",
    "A = np.array([[4, 0],\n",
    "              [0, 3],\n",
    "              [2, 2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Singular Value Decomposition\n",
    "U, S, Vt = np.linalg.svd(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"U matrix:\")\n",
    "print(U)\n",
    "print(\"\\nΣ matrix:\")\n",
    "print(np.diag(S))\n",
    "print(\"\\nV* matrix:\")\n",
    "print(Vt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'A', 'U', 'S', and 'Vt' are defined as per SVD requirements\n",
    "# Convert the 1D array of singular values to a full diagonal matrix\n",
    "Sigma = np.zeros((U.shape[1], Vt.shape[0]))\n",
    "np.fill_diagonal(Sigma, S)\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(25, 5), constrained_layout=True)\n",
    "\n",
    "sns.heatmap(A, ax=axs[0], cmap=\"viridis\", annot=True, cbar=False)\n",
    "axs[0].set_title(\"Matrix A\")\n",
    "\n",
    "sns.heatmap(U, ax=axs[1], cmap=\"viridis\", annot=True, cbar=False)\n",
    "axs[1].set_title(\"Left Singular Vectors U\")\n",
    "\n",
    "sns.heatmap(Sigma, ax=axs[2], cmap=\"viridis\", annot=True, cbar=False)\n",
    "axs[2].set_title(\"Diagonal Matrix Σ\")\n",
    "\n",
    "sns.heatmap(Vt, ax=axs[3], cmap=\"viridis\", annot=True, cbar=False)\n",
    "axs[3].set_title(\"Right Singular Vectors V*\")\n",
    "\n",
    "sns.heatmap(np.dot(U, np.dot(Sigma, Vt)), ax=axs[4], cmap=\"viridis\", annot=True, cbar=False)\n",
    "axs[4].set_title(\"U x Σ x V*\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zx-JISotqvxQ"
   },
   "source": [
    "#### Real Life Use Case of SVD in Machine Learning.\n",
    "\n",
    "Applying Singular Value Decomposition (SVD) to real-world datasets for dimensionality reduction is a common and powerful technique used in various fields such as image processing, natural language processing (NLP), and recommendation systems. Dimensionality reduction can help reduce the computational complexity of a dataset while preserving its essential characteristics. In this example, we'll demonstrate how to apply SVD for dimensionality reduction using a real-world dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 17 Dimensionality Reduction using Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Display the first few images\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 4))\n",
    "for ax, image, label in zip(axes, X, y):\n",
    "    ax.imshow(image.reshape(8, 8), cmap=plt.cm.gray_r)\n",
    "    ax.set_title(f'Label: {label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply SVD and reduce the dimensionality\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Reduce dimensions to 2 using SVD\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "X_reduced = svd.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Visualize the original and reduced data\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, edgecolor='none', alpha=0.7, cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title('2D Projection of Handwritten Digits using SVD')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bxp8yl3M1yc"
   },
   "source": [
    "### Case Study : Linear Algebra In Machine Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dataset\n",
    "sizes = np.array([650, 785, 1200, 720, 975])\n",
    "prices = np.array([77250, 92850, 135000, 86000, 110500])\n",
    "\n",
    "# Visualize the data\n",
    "plt.scatter(sizes, prices, color='blue')\n",
    "plt.xlabel('Size (sq.ft)')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('House Prices based on Size')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input matrix X and target vector y\n",
    "X = np.vstack([np.ones(sizes.shape[0]), sizes]).T\n",
    "y = prices\n",
    "\n",
    "# Calculate the coefficients using the normal equation\n",
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "print(f\"Intercept: {beta[0]:.2f}\")\n",
    "print(f\"Slope (Price per sq.ft): {beta[1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict prices using the linear model\n",
    "predicted_prices = X @ beta\n",
    "\n",
    "# Visualize the original data and the best-fitting line\n",
    "plt.scatter(sizes, prices, color='blue', label='Actual Prices')\n",
    "plt.plot(sizes, predicted_prices, color='red', label='Predicted Prices')\n",
    "plt.xlabel('Size (sq.ft)')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('House Prices Prediction based on Size')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revised Date: November 4, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
