{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7967261a",
   "metadata": {},
   "source": [
    "# IE6400 Foundations of Data Analytics Engineering\n",
    "# Fall 2023 \n",
    "### Module 3: Clustering Methods Part - 1\n",
    "#### - STUDENT VERSION -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1150fd",
   "metadata": {},
   "source": [
    "### Proximity Measures\n",
    "\n",
    "Proximity measures are metrics used to determine the similarity or dissimilarity between data points. They play a crucial role in various machine learning and data analysis techniques, especially clustering and classification. The choice of a proximity measure often depends on the nature of the data and the specific problem at hand.\n",
    "\n",
    "#### Types of Proximity Measures\n",
    "\n",
    "There are two main types of proximity measures:\n",
    "\n",
    "1. **Similarity Measures**: These quantify how similar two data points are. Higher values indicate greater similarity.\n",
    "2. **Dissimilarity Measures (or Distance Measures)**: These represent the \"distance\" or dissimilarity between two data points. Higher values indicate greater dissimilarity.\n",
    "\n",
    "#### Common Proximity Measures\n",
    "\n",
    "#### For Continuous Data:\n",
    "\n",
    "1. **Euclidean Distance**: \n",
    "    - It's the \"ordinary\" straight-line distance between two points in Euclidean space.\n",
    "\n",
    "    $d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$\n",
    "\n",
    "\n",
    "2. **Manhattan Distance (or L1 norm)**:\n",
    "    - It's the distance between two points measured along axes at right angles (taxicab or city block distance).\n",
    "\n",
    "    $d(x, y) = \\sum_{i=1}^{n} |x_i - y_i|$\n",
    "\n",
    "\n",
    "3. **Minkowski Distance**:\n",
    "    - A generalized metric. When \\(p=2\\), it becomes the Euclidean distance. When \\(p=1\\), it's the Manhattan distance.\n",
    "\n",
    "    $d(x, y) = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{\\frac{1}{p}}$\n",
    "\n",
    "\n",
    "#### For Categorical Data:\n",
    "\n",
    "1. **Hamming Distance**: \n",
    "    - Used for categorical variables. It's the number of positions at which the corresponding symbols in two strings of equal length are different.\n",
    "\n",
    "2. **Jaccard Similarity**:\n",
    "    - Measures the similarity between two sets. It's the size of the intersection divided by the size of the union of the two sets.\n",
    "\n",
    "    $J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$\n",
    "\n",
    "\n",
    "### For Mixed-Type Data:\n",
    "\n",
    "1. **Gower Distance**: \n",
    "    - Combines various distance metrics for mixed-type data.\n",
    "\n",
    "### For Binary Data:\n",
    "\n",
    "1. **Jaccard Coefficient**: \n",
    "    - Similar to Jaccard similarity but specifically tailored for binary attributes.\n",
    "\n",
    "2. **Cosine Similarity**:\n",
    "    - Measures the cosine of the angle between two non-zero vectors. It's often used in text analysis to determine similarity between documents.\n",
    "\n",
    "    $\\text{cosine_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ec733",
   "metadata": {},
   "source": [
    "#### Exercise 1 Understanding Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dataset\n",
    "customers = np.array([[5, 3], [2, 8], [9, 1], [4, 7]])\n",
    "\n",
    "# Visualize the data\n",
    "plt.scatter(customers[:, 0], customers[:, 1], color='blue', label='Customers')\n",
    "plt.xlabel('Product A')\n",
    "plt.ylabel('Product B')\n",
    "plt.title('Purchase History of Customers')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute Euclidean distance\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "\n",
    "# Calculate the distance between Customer 1 and Customer 2\n",
    "distance = euclidean_distance(customers[0], customers[1])\n",
    "\n",
    "print(f\"Euclidean Distance between Customer 1 and Customer 2: {distance:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90730bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data with the Euclidean distance\n",
    "plt.scatter(customers[:, 0], customers[:, 1], color='blue', label='Customers')\n",
    "plt.plot([customers[0][0], customers[1][0]], [customers[0][1], customers[1][1]], 'ro-')\n",
    "plt.xlabel('Product A')\n",
    "plt.ylabel('Product B')\n",
    "plt.title('Euclidean Distance between Customer 1 and Customer 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92999d2",
   "metadata": {},
   "source": [
    "#### Exercise 2 Understanding Manhattan Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e6894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dataset\n",
    "rides = {\n",
    "    'Start': [(2, 3), (1, 4), (3, 3), (6, 1)],\n",
    "    'End': [(5, 6), (4, 2), (3, 7), (2, 5)]\n",
    "}\n",
    "\n",
    "# Visualize the data\n",
    "for start, end in zip(rides['Start'], rides['End']):\n",
    "    plt.plot([start[0], end[0]], [start[1], end[1]], 'ro-')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.title('Taxi Rides')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d6a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute Manhattan distance\n",
    "def manhattan_distance(point1, point2):\n",
    "    return abs(point1[0] - point2[0]) + abs(point1[1] - point2[1])\n",
    "\n",
    "# Calculate the distance for Ride 1\n",
    "distance = manhattan_distance(rides['Start'][0], rides['End'][0])\n",
    "\n",
    "print(f\"Manhattan Distance for Ride 1: {distance} blocks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d7fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data with the Manhattan distance for Ride 1\n",
    "start, end = rides['Start'][0], rides['End'][0]\n",
    "plt.plot([start[0], end[0]], [start[1], start[1]], 'bo-')\n",
    "plt.plot([end[0], end[0]], [start[1], end[1]], 'bo-')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.title('Manhattan Distance for Ride 1')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87610d06",
   "metadata": {},
   "source": [
    "#### Exercise 3 Understanding Chebyshev Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dataset\n",
    "moves = {\n",
    "    'Start': [(2, 3), (1, 4), (3, 3), (6, 1)],\n",
    "    'Target': [(5, 6), (4, 2), (3, 7), (2, 5)]\n",
    "}\n",
    "\n",
    "# Visualize the data\n",
    "for start, target in zip(moves['Start'], moves['Target']):\n",
    "    plt.plot([start[0], target[0]], [start[1], target[1]], 'ro-')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.title('Game Moves')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute Chebyshev distance\n",
    "def chebyshev_distance(point1, point2):\n",
    "    return max(abs(point1[0] - point2[0]), abs(point1[1] - point2[1]))\n",
    "\n",
    "# Calculate the distance for Move 1\n",
    "distance = chebyshev_distance(moves['Start'][0], moves['Target'][0])\n",
    "\n",
    "print(f\"Chebyshev Distance for Move 1: {distance} squares\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18cb66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data with the Chebyshev distance for Move 1\n",
    "start, target = moves['Start'][0], moves['Target'][0]\n",
    "plt.scatter(*zip(*[start, target]), color=['blue', 'red'])\n",
    "plt.plot([start[0], target[0]], [start[1], start[1]], 'g--')\n",
    "plt.plot([target[0], target[0]], [start[1], target[1]], 'g--')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.title('Chebyshev Distance for Move 1')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa66f7",
   "metadata": {},
   "source": [
    "#### Exercise 4 Understanding Minkowski Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample Dataset\n",
    "points = np.array([[2, 3], [3, 5]])\n",
    "\n",
    "# Minkowski Distance Function\n",
    "def minkowski_distance(p1, p2, p):\n",
    "    return np.sum(np.abs(p1 - p2) ** p) ** (1/p)\n",
    "\n",
    "# Calculate Minkowski Distance for p=1,2,3,4\n",
    "p_values = [1, 2, 3, 4]\n",
    "distances = [minkowski_distance(points[0], points[1], p) for p in p_values]\n",
    "\n",
    "distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648dcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, d in zip(p_values, distances):\n",
    "    print(f\"For p = {p}, Minkowski Distance = {d:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41887a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p_values, distances, 'o-', color='blue')\n",
    "plt.xlabel('Value of p')\n",
    "plt.ylabel('Minkowski Distance')\n",
    "plt.title('Minkowski Distance for Different p Values')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbcc85e",
   "metadata": {},
   "source": [
    "#### Exercise 5 Understanding the Dissimilarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "# Sample Dataset\n",
    "data = np.array([[2, 3], [3, 5], [5, 8], [8, 9], [7, 5]])\n",
    "\n",
    "# Compute Dissimilarity Matrix\n",
    "dissimilarity = distance_matrix(data, data)\n",
    "\n",
    "dissimilarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba76838",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dissimilarity, annot=True, cmap='YlGnBu', cbar=True)\n",
    "plt.title('Dissimilarity Matrix Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfac6c6",
   "metadata": {},
   "source": [
    "#### Exercise 6 Understanding the Hamming Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample Dataset\n",
    "strings = [\"101010\", \"100010\", \"111011\", \"101011\", \"110010\"]\n",
    "\n",
    "# Compute Hamming Distance\n",
    "def hamming_distance(s1, s2):\n",
    "    return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))\n",
    "\n",
    "distances = [hamming_distance(strings[0], s) for s in strings]\n",
    "\n",
    "distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defc1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(strings, distances, color='skyblue')\n",
    "plt.xlabel('Strings')\n",
    "plt.ylabel('Hamming Distance')\n",
    "plt.title('Hamming Distance from the First String')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc13dfb",
   "metadata": {},
   "source": [
    "#### Exercise 7 Understanding the Jaccard Similarity for Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a7e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generating a sample dataset\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Group1': np.random.choice(['Apple', 'Banana', 'Cherry', 'Date'], 100),\n",
    "    'Group2': np.random.choice(['Apple', 'Banana', 'Cherry', 'Date'], 100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1512a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Determine the unique categories chosen by each group\n",
    "group1_unique = set(df['Group1'].unique())\n",
    "group2_unique = set(df['Group2'].unique())\n",
    "\n",
    "# Step 2: Compute the intersection of the two sets\n",
    "intersection = group1_unique.intersection(group2_unique)\n",
    "\n",
    "# Step 3: Compute the union of the two sets\n",
    "union = group1_unique.union(group2_unique)\n",
    "\n",
    "# Step 4: Calculate the Jaccard Similarity\n",
    "jaccard_similarity = len(intersection) / len(union)\n",
    "jaccard_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b875a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge matplotlib-venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77fe6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "# Plotting the unique categories for each group and their intersection\n",
    "venn2_subsets = (len(group1_unique - group2_unique), \n",
    "                 len(group2_unique - group1_unique), \n",
    "                 len(intersection))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "venn2(subsets=venn2_subsets, set_labels=('Group1', 'Group2'))\n",
    "plt.title(\"Venn Diagram of Preferences for Group1 and Group2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ef0b3",
   "metadata": {},
   "source": [
    "#### Exercise 8 Understanding the Gower Distance for Mixed-Type Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generating a sample dataset\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Age': np.random.randint(20, 60, 100),\n",
    "    'Income': np.random.randint(30000, 80000, 100),\n",
    "    'Fruit_Preference': np.random.choice(['Apple', 'Banana', 'Cherry'], 100),\n",
    "    'Is_Smoker': np.random.choice([True, False], 100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e9fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d0ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gower\n",
    "\n",
    "# Compute the Gower Distance matrix\n",
    "gower_distance_matrix = gower.gower_matrix(df)\n",
    "\n",
    "# Display a portion of the Gower Distance matrix\n",
    "gower_distance_matrix[:5, :5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af82a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(gower_distance_matrix, cmap='viridis', cbar=True)\n",
    "plt.title(\"Gower Distance Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39acff9",
   "metadata": {},
   "source": [
    "#### Exercise 9 Understanding the Jaccard Coefficient for Binary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generating a sample dataset with binary attributes\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Bought_Apple': np.random.choice([0, 1], 100),\n",
    "    'Bought_Banana': np.random.choice([0, 1], 100),\n",
    "    'Bought_Cherry': np.random.choice([0, 1], 100),\n",
    "    'Is_Vegetarian': np.random.choice([0, 1], 100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc323ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "# Compute the Jaccard Coefficient for the first two data points as an example\n",
    "data_point_1 = df.iloc[0]\n",
    "data_point_2 = df.iloc[1]\n",
    "\n",
    "jaccard_coefficient = jaccard_score(data_point_1, data_point_2, average='macro')\n",
    "\n",
    "jaccard_coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a83a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_coefficients = []\n",
    "\n",
    "# Compute Jaccard Coefficients for all pairs of data points\n",
    "for i in range(len(df)):\n",
    "    for j in range(i+1, len(df)):\n",
    "        coef = jaccard_score(df.iloc[i], df.iloc[j], average='macro')\n",
    "        jaccard_coefficients.append(coef)\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(jaccard_coefficients, bins=20, edgecolor='k', alpha=0.7)\n",
    "plt.title(\"Distribution of Jaccard Coefficients\")\n",
    "plt.xlabel(\"Jaccard Coefficient\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0feb2a",
   "metadata": {},
   "source": [
    "#### Exercise 10 Understanding the Cosine Similarity for Binary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generating a sample dataset with binary attributes\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Bought_Apple': np.random.choice([0, 1], 100),\n",
    "    'Bought_Banana': np.random.choice([0, 1], 100),\n",
    "    'Bought_Cherry': np.random.choice([0, 1], 100),\n",
    "    'Is_Vegetarian': np.random.choice([0, 1], 100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5758398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute the cosine similarity for the first two data points as an example\n",
    "data_point_1 = df.iloc[0].values.reshape(1, -1)\n",
    "data_point_2 = df.iloc[1].values.reshape(1, -1)\n",
    "\n",
    "cosine_sim = cosine_similarity(data_point_1, data_point_2)\n",
    "\n",
    "cosine_sim[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ccd8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = []\n",
    "\n",
    "# Compute cosine similarities for all pairs of data points\n",
    "for i in range(len(df)):\n",
    "    for j in range(i+1, len(df)):\n",
    "        coef = cosine_similarity(df.iloc[i].values.reshape(1, -1), df.iloc[j].values.reshape(1, -1))\n",
    "        cosine_similarities.append(coef[0][0])\n",
    "\n",
    "# Plotting the histogram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(cosine_similarities, bins=20, edgecolor='k', alpha=0.7)\n",
    "plt.title(\"Distribution of Cosine Similarities\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47b6d7",
   "metadata": {},
   "source": [
    "### Evaluating Clustering Methods\n",
    "\n",
    "Evaluating the results of clustering methods is critical to understand the quality and relevance of the clusters formed. Since clustering is unsupervised, assessing its effectiveness can be somewhat subjective. However, there are established metrics and techniques to guide this evaluation, both when ground truth labels are available and when they aren't.\n",
    "\n",
    "#### Internal Evaluation:\n",
    "\n",
    "Without ground truth labels, evaluate clustering based on the dataset's intrinsic properties.\n",
    "\n",
    "#### Metrics:\n",
    "\n",
    "1. **Silhouette Coefficient**:\n",
    "   - Compares similarity of data points to their own cluster against other clusters.\n",
    "   - Values range from -1 (incorrect clustering) to 1 (highly dense clustering), with 0 suggesting overlapping clusters.\n",
    "\n",
    "2. **Davies-Bouldin Index**:\n",
    "   - A ratio of within-cluster and between-cluster distances.\n",
    "   - Lower values indicate better clustering.\n",
    "\n",
    "3. **Calinski-Harabasz Index**:\n",
    "   - Compares between-cluster dispersion to within-cluster dispersion.\n",
    "   - Higher values indicate better-defined clusters.\n",
    "\n",
    "4. **Dunn Index**:\n",
    "   - Ratio of the smallest distance between points in different clusters to the largest intra-cluster distance.\n",
    "   - Higher values indicate better clustering.\n",
    "\n",
    "#### Relative Evaluation:\n",
    "\n",
    "This involves comparing the results of clustering for different configurations or numbers of clusters.\n",
    "\n",
    "#### Techniques:\n",
    "\n",
    "1. **Elbow Method**:\n",
    "   - Used with K-means to determine the optimal number of clusters.\n",
    "   - Plot the variance explained (or inertia) against the number of clusters. The \"elbow\" point, where the rate of decrease sharply changes, often indicates an optimal number of clusters.\n",
    "\n",
    "#### Stability and Consistency:\n",
    "\n",
    "Evaluate the robustness of clusters by perturbing the dataset.\n",
    "\n",
    "#### Techniques:\n",
    "\n",
    "1. **Sub-sampling or Bootstrapping**:\n",
    "   - Repeatedly sample subsets of data and perform clustering.\n",
    "   - Examine the consistency of the clustering results.\n",
    "\n",
    "2. **Adding Noise**:\n",
    "   - Introduce random noise to the data.\n",
    "   - Stable clusters should remain relatively unchanged.\n",
    "\n",
    "#### Challenges and Considerations:\n",
    "\n",
    "1. **Subjectivity**: Without a definitive \"correct\" clustering, some evaluation aspects remain subjective.\n",
    "2. **Scale Sensitivity**: Some metrics need data normalization or standardization.\n",
    "3. **Choice of Metric**: Different metrics might give varied evaluations for the same clustering result.\n",
    "\n",
    "In conclusion, while evaluating clustering methods, it's often beneficial to consider multiple metrics and, when possible, combine them with domain knowledge to get a comprehensive view of the clustering quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081aa6ff",
   "metadata": {},
   "source": [
    "#### Exercise 11 Evaluating Clustering with Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c9c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da79f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Applying KMeans clustering\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "predicted_clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Calculating the Silhouette Coefficient\n",
    "sil_coeff = silhouette_score(X, predicted_clusters, metric='euclidean')\n",
    "\n",
    "sil_coeff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c587aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=predicted_clusters, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X')\n",
    "plt.title(\"Clusters Formed by KMeans\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa64751f",
   "metadata": {},
   "source": [
    "#### Exercise 12 Evaluating Clustering with Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e6ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b82377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "# Applying KMeans clustering\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "predicted_clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Calculating the Davies-Bouldin Index\n",
    "dbi = davies_bouldin_score(X, predicted_clusters)\n",
    "\n",
    "dbi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878dca0",
   "metadata": {},
   "source": [
    "#### Exercise 13 Evaluating Clustering with Calinski-Harabasz Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0389b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606251f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "# Applying KMeans clustering\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "predicted_clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Calculating the Calinski-Harabasz Index\n",
    "chi = calinski_harabasz_score(X, predicted_clusters)\n",
    "\n",
    "chi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56d60c3",
   "metadata": {},
   "source": [
    "#### Exercise 14 Evaluating Clustering with Dunn Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "# Applying KMeans clustering\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "predicted_clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Calculating the Dunn Index\n",
    "def dunn_index(X, labels):\n",
    "    pairwise_dists = pairwise_distances(X)\n",
    "    min_intercluster_distance = np.min([pairwise_dists[labels == i][:, labels == j].min() for i in np.unique(labels) for j in np.unique(labels) if i != j])\n",
    "    max_diameter = max([np.max(pairwise_distances(X[labels == i])) for i in np.unique(labels)])\n",
    "    \n",
    "    return min_intercluster_distance / max_diameter\n",
    "\n",
    "di = dunn_index(X, predicted_clusters)\n",
    "\n",
    "di\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b28f4",
   "metadata": {},
   "source": [
    "#### Exercise 15 Validating Clustering using the Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e8c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Applying KMeans clustering for a range of k values\n",
    "wcss = []  # Within-Cluster-Sum-of-Squares\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the Elbow Method graph\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1, 11), wcss, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947499e",
   "metadata": {},
   "source": [
    "#### Exercise 16 Validating Clustering using Cohesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32043174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83643a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Applying KMeans clustering\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Computing the cohesion value\n",
    "cohesion = kmeans.inertia_\n",
    "\n",
    "cohesion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33493dba",
   "metadata": {},
   "source": [
    "#### Exercise 17 Validating Clustering using Separation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a sample dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Visualizing the generated dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.title(\"Generated Data Points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aed7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Applying KMeans clustering\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Computing the separation score\n",
    "centroids = kmeans.cluster_centers_\n",
    "separation = np.sum(np.var(centroids, axis=0))\n",
    "\n",
    "separation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd64d16",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87287638",
   "metadata": {},
   "source": [
    "#### Revised Date: October 23, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc18ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
